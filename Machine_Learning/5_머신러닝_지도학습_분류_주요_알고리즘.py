# -*- coding: utf-8 -*-
"""5.머신러닝_지도학습_분류_주요_알고리즘.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J4Ll03qe0E2m-y2FJvZ2UtCoUIhXEL2L

# 결정트리

## 개념

- 데이터에 들어있는 규칙을 학습을 통해 찾고 **트리 기반**으로 **분류의 규칙을 만들어 내는 알고리즘**
- 성능
  - 어떤 기준의 규칙인가 : 규칙을 만드는 룰, 임계값
  - 분류를 하는 규칙을 어떻게 효율적으로 배치할 것인가
  - 깊이는 어느정도로 할 것인가

## 형태
"""

from IPython.display import Image

# 결정트리 형태
Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_결정트리.jpg')

"""- 노드
  - 루트노드 : 데이터의 시작
  - 규칙노드 : 데이터의 분기점, feature(특성, 독립변수)들을 조합해 규칙을 생성하고 그 기준으로 데이터를 분기하는 노드, 결과로 브랜치(가지)가 되서 서브트리가 만들어짐
  - 리프노드 : 데이터의 끝, 더이상 분기점이 없음, 결정된 class(종속변수)값을 가짐, 리프노드의 수가 클래스의 수를 의미하지는 않음
- 브랜치 / 서브트리 : 새로운 규칙이 조건에 의해 생성되면 서브트리가 생성됐다고 표현
- 깊이 : 깊을수록 정확도는 상승하지만 성능 저하와 과적합의 원인이 될 수 있음, 이를 위해 임계값이 필요함
- 목표 지점
  - 데이터가 균일하게 분류되도록 분할 처리하는 것
  - 데이터의 균일성을 스케일링을 통해 처리하는 것
  - 과적합에 대한 대비, 깊이감 조절, 데이터의 쏠림 방지
"""

# 결정트리 시각화
Image('/content/drive/MyDrive/머신러닝/0404_res/today/ml-의사결정트리.png')

"""## 적용"""

# 라이브러리
# 결정트리
from sklearn.tree import DecisionTreeClassifier
# 데이터 세트
from sklearn.datasets import load_iris
# 학습, 검증 나누기
from sklearn.model_selection import train_test_split

"""### DecisionTreeClassifier의 하이퍼 파라미터
   - criterion
    - 개요 : 결정트리에서는 데이터가 균일하도록 데이터 세트가 선택되게 만드는 것이 중요함
    - 지표
      - gini : 불평등지수(지니계수같은거)
        - 0에 가까우면 평등, 1에 가까우면 불평등
        - 분류된 데이터가 다양하면 평등, 특정값에 쏠리면 불평등
      - entropy : 엔트로피를 이용한 정보이득지수(infomation gain)
        - 주어진 데이터 집합의 혼잡함를 표현
        - 데이터의 종류가 많으면 엔트로피가 높다
        - 데이터의 종류가 적으면 엔트로피가 낮다
        - 정보이득지수 = 1 - 엔트로피 지수 값
           - 판단 : 정보이득지수가 임계값(설정값)보다 크다면 규칙노드로 판단이 됨 -> 브랜치(가지치기) 실행
"""

# criterion의 하이퍼 파라미터 gini, entropy 관점 해석
Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_결정트리2.jpg')
# 아래 그림을
# 지니계수 관점에서 해석
# A세트 : 여러 데이터가 섞여 있어서 평등하다
# C세트 : 데이터가 쏠려있어서 불평등하다

# 엔트로피 관점에서 해석
# A세트 : 여러 데이터가 섞여 있어서 혼잡하다
# C세트 : 데이터가 쏠려 있어서 균일하다

# 브랜치(가지치기) 원리
Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_결정트리3.jpg')

# 2 - 1번 : 더이상 분류 안 함 -> 리프노드가 생성됨
# 2 - 2번 : 규칙노드를 만들어야 함 -> 기준이 필요(지니 또는 엔트로피를 지정)
# -> 기준을 만족하면 브랜치

"""- 과적합 방지 하이퍼 파라미터
  - 깊이는 어느정도로 할 것인가
  - 리프노드를 몇 개로 할 것인가
  - 규칙노드로 브랜치를 할 때 조건을 어떻게 할 것인가
"""

# 하이퍼 파라미터 종류들
Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_결정트리4.jpg')

# min_samples_split : 규칙노드가 만들어지는 조건으로 몊 개의 샘플 데이터가 나눌 것인가
# 값이 작을수록 튀는 값을 잡아낼 수 있지만 과적합이 될 수도 있음
# min_samples_leaf : 리프노드가 되기 위해 해당 노드에서 최소 몇 개의 샘플을 가지고 있는가
# 최소 이정도 샘플은 있어야 리프노드가 됨

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_결정트리5.jpg')

# max_features
# max_depth
# max_leaf_nodes

# 알고리즘 생성
# 파라미터에서는 규칙 노드를 만드는 판단 기준(지니 혹은 엔트로피)에 대해 사용여부만 지정하고 임계값 접근은 미지정
# 튜닝은 지니 혹은 엔트로피 둘 중 어느쪽에서 더 성능이 나오는지 확인
clf = DecisionTreeClassifier(random_state = 0)
clf

# 데이터 준비
tmp = load_iris()

X_train, X_test, y_train, y_test = train_test_split(tmp.data, tmp.target, test_size = 0.25, random_state = 0)

# 확인
X_train.shape, X_test.shape, y_train.shape, y_test.shape

# 학습
clf.fit(X_train, y_train)

# 예측 및 성능평가
clf.score(X_test, y_test)

"""- 결정트리가 어떤 과정을 거쳐서 노드를 분류, 생성했는지 확인하기"""

# 모델을 덤프해서 데이터, 정답을 같이 표기해서 다시 덤프하는 라이브러리
from sklearn.tree import export_graphviz

# export_graphviz(모델명, 덤프할 때의 파일명, 데이터의 컬럼명, 정답 이름)
export_graphviz(clf,
                out_file = 'dtree.model',
                feature_names = tmp.feature_names,
                class_names = tmp.target_names,
                filled = True)

# 위에서 덤프한 데이터 읽기
with open('dtree.model') as f :
  mGraph = f.read()

# 덤프한 데이터를 시각화하는 라이브러리
import graphviz

# 시각화하기
graphviz.Source(mGraph)

"""- 결정트리의 규칙노드들을 체크해보면 사용하지 않은 특성이 보임
- 사용 비중이 차이가 나는 것을 알 수 있음
- 결정트리의 분류룰 완성할 때 특성별 기여도가 다르다는 것을 의미
"""

# 시각화를 통해 중요 특성을 표현해보기
# 라이브러리
import seaborn as sns
import numpy as np

# 특성별 기여도 확인
clf.feature_importances_

# 반올림하기
f1 = np.round(clf.feature_importances_, 3)
f1

# 바플롯 그리기
sns.barplot(x = f1, y = tmp.feature_names)

# sepal length 컬럼은 분류시 기여도가 거의 없음
# 제외해도 결정트리 알고리즘에서는 문제 없음
# (알고리즘이 변경되면 달라질 수 있음)

"""- 과적합 처리
  - 기존 데이터 말고 새 데이터를 사용함
  - make_xxxx 계열 함수 사용
"""

# Commented out IPython magic to ensure Python compatibility.
# 라이브러리
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

# %matplotlib inline

# 더미데이터 생성
X, y = make_classification(n_features = 2, # 특성(독립변수)의 총 수
                           n_informative = 2, # 특성 중 정답(종속변수)과 상관관계가 있는 성분의 수
                           n_redundant = 0, # 특성 중 다른 특성과 선형 조합을 나타낼 수 있는 수
                           n_classes = 3, # 정답의 수
                           n_clusters_per_class = 1, # 정답 별 군집(클러스터)의 개수
                           random_state = 0) # 난수 설정
X.shape, y.shape

# 산점도
# x축 : 첫번째 값 집합
# y축 : 두번쩨 값 집합
plt.scatter(X[:, 0], X[:, 1], s = 25, marker = 'o', c = y, cmap = 'rainbow', edgecolors = 'k')

# 결정트리 관점에서 보면 청록색 데이터는 바로 분류 가능함
# 다른색(빨간색, 보라색)은 몇번의 규칙노드를 통해 영역을 분리할 수 있음

# 밑의 3번의 시작값, 끝값을 특정 기준값 간격으로 구간나누기를 진행하려면 넘파이 필요함 
import numpy as np

# 등고선 그래프 함수 만들어보기
def show_rect_clf_boundary(model, X, y) :
  '''
  model : 알고리즘(분류기)
  X : 특성, 독립변수
  y : 정답, 종속변수
  '''
  # 0. 차트 외관에 필요한 요소 준비
  _, ax = plt.subplots()

  # 1. 산점도 그리기
  ax.scatter(X[:, 0], X[:, 1], s = 25, marker = 'o', c = y, cmap = 'rainbow',
             edgecolors = 'k', clim = (y.min(), y.max()), zorder = 3)
  
  # 2. 차트 외곽 정리
  ax.axis('off') # 축 정보 삭제
  ax.axis('tight') # 시각화시 밀집된 데이터의 간격을 벌여줌 

  # 3. 면적(같은 답을 가진 사각 영역)에 대한 데이터 준비
  # 화면상의 픽셀을 구함(x(독립변수 1), y(독립변수 2) 축의 좌표)
  # x, y 좌표계의 시작값, 끝값 변수에 저장하고 확인
  xlim_start, xlim_end = ax.get_xlim()
  ylim_start, ylim_end = ax.get_ylim()

  # 구간을 200개로 분리
  x_std = np.linspace(xlim_start, xlim_end, num = 200)
  y_std = np.linspace(ylim_start, ylim_end, num = 200)

  xx, yy = np.meshgrid(x_std, y_std)

  tmp_xy = np.c_[xx.ravel(), yy.ravel()]
  print(tmp_xy)

  ax.scatter(tmp_xy[:, 0], tmp_xy[:, 1], s = 1, zorder = 1)

  # 4. 알고리즘 생성(clf, 이미 완료됨)
  # 5. 학습, 예측
  clf.fit(X, y)
  # 6. 위의 좌표 데이터를 이용해 영역 예측하기
  Z = clf.predict(tmp_xy)
  # 정답이 3개라서 영역도 3개 나와야 함
  print(np.unique(Z))

  # 7. 등고선 그래프를 그리기 위한 작업(분류된 영역 그리기)
  # xx, yy는 2차원이고 Z는 1차원이라서 한번에는 못 해서 Z를 2차원으로 만들어야 함
  ax.contourf(xx, yy, # xx, yy 넣어주기
              Z.reshape(xx.shape), # Z를 xx와 같은 모양으로 만들어줌
              # 여기부터는 데코레이션
              alpha = 1.0,
              levels = np.arange(len(np.unique(Z)) + 1) - 0.5, # 면적 색상
              cmap = 'rainbow',
              clim = (y.min(), y.max()),
              zorder = 1)

# 함수 호출
show_rect_clf_boundary(DecisionTreeClassifier(), X, y)

# 결과
# 과적합된 모델의 분류 그래프를 통해 초록색에 과적합, 편향적인 것을 확인

"""# 앙상블

## 개념

- Ensemble learning(앙상블 학습법)
- 여러개의 알고리즘(분류)을 생성(동일, 서로 다른)하여 학습을 수행하고 그 예측값을 통합(평균 등)해 최종 예측을 수행하는 기법
- 약한 알고리즘들을 뭉쳐서 강한 모델을 만들 수 있음
- 단일 분류기(모델 1개)보다 높은 성능/신뢰도를 얻을 수도 있음
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블1.jpg')
# 앙상블 종류
# 보팅 : 서로 다른 알고리즘을 여러개 사용
# 배깅 : 동일 알고리즘을 여러개 사용

"""## 보팅(Voting)

- 서로 다른 알고리즘 사용
- 종류
  - 하드 보팅
    - 다수로 결정된 예측값을 보팅의 결과로 종합하는 방식
    - 다수결에 의존해서 질적인 부분(실제 비중)은 고려하지 못 하는 문제가 발생
  - 소프트 보팅
    - 정답이 결정되는 모든 확률을 더해서 평균을 내 높은쪽으로 결론을 내는 방식
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블2_보팅.jpg')
# 보팅 개념

"""### 보팅 구현 실습"""

# 알고리즘
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

# 보팅
from sklearn.ensemble import VotingClassifier

# 데이터
from sklearn.datasets import load_breast_cancer

# 학습용, 검증용 데이터 분리
from sklearn.model_selection import train_test_split

# 평가도구
from sklearn.metrics import accuracy_score

# 데이터 처리
import pandas as pd

# 데이터 불러오기
cancer = load_breast_cancer()

# 데이터 프레임으로 만들기
data_df = pd.DataFrame(cancer.data, columns = cancer.feature_names)
data_df.shape

# 데이터를 학습용, 테스트용으로 분리하기
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,
                                                    test_size = 0.25, random_state = 0)
X_train.shape, X_test.shape

# 보팅에 사용할 알고리즘 생성하기
lr_clf = LogisticRegression()
knn_clf = KNeighborsClassifier(n_neighbors = 8)

# 보팅에 사용할 알고리즘 연결하기
# 소프트 방식은 알고리즘의 개수가 짝수개라도 동수가 나올 확률은 낮음
vo_clf = VotingClassifier(estimators = [
  ('LR', lr_clf),
  ('KNN', knn_clf)
],
voting = 'soft')

# 학습하기
# n개(여기서는 2개)의 모델이 각각 학습함
vo_clf.fit(X_train, y_train)

# 예측
y_pred = vo_clf.predict(X_test)

# 성능평가
accuracy_score(y_test, y_pred)

# 보팅에 참여한 개별 알고리즘은 어떤 결과였을지 확인해보기
for clf in [LogisticRegression(), KNeighborsClassifier(n_neighbors = 8)] :
  clf.fit(X_train, y_train)
  v_pred = clf.predict(X_test)
  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))

# 결과
# 로지스틱 회귀가 성능이 더 좋아서 보팅 예측 결과에 영향을 미친 것으로 추측됨

"""### 앙상블 학습의 핵심

- 트레이드 - 오프 문제
  - A가 높으면 B가 낮은 상반성을 가진 개념들 사이의 문제
  - 상반된 2개의 개념을 적절하게 조정해야 트레이드 - 오프 문제를 방지할 수 있음
  - 앙상블의 트레이드 - 오프
    - 편향 : 해당 값이 높으면 그 문제는 잘 맞추지만 다른 문제는 못 맞추는 문제 발생 -> 과소적합의 문제를 야기할 수 있음
    - 분산 : 분산이 높으면 이상치를 잘 잡아내지만 과대적합의 문제를 야기할 수 있음

### 부트 스크래핑

- cv(학습용 데이터를 학습용과 검증용으로 나눠서 처리하는 방식)는 앙상블 학습에서 어떻게 적용되는가
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블2_배깅_부트스트래핑.jpg')

"""## 배깅(Bagging)

- 동일 알고리즘을 여러개 만들어서 보팅을 수행함
- 대표 알고리즘 : 랜덤 포레스트
  - (앙상블 기반들 중)뛰어난 성능, 빠른 수행시간, 유연성
  - 기초 알고리즘으로는 결정트리를 사용함
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블2_배깅_랜덤포레스트.jpg')
# 배깅 개념
# 알고리즘 3개 사용
# 데이터 세트에는 중복된 데이터가 들어갈 수 있음 -> 부트 스트래핑
# 동일한 3개의 알고리즘은 각각 다른 하이퍼 파라미터를 사용하여 서로 다른 데이터를 가지고 학습/예측을 수행
# 개별 확률을 더하고 평균을 내서 분류를 수행함 -> 소프트 보팅

# 랜덤 포레스트는 이런 개념을 모두 포괄한 알고리즘임

"""### 랜덤 포레스트
- 집단 학습 기반
- 구성에 따라서는 고밀도 정밀 분류, 회귀, 클러스터링 모두 작업 가능
- 하드 / 소프트 보팅 가능
- 학습 데이터를 무작위로 샘플링을 수행 -> 랜덤
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/ml-랜덤포레스트.png')

"""### 배깅 - 랜덤 포레스트 실습
- 위에서 사용한 데이터 활용함
"""

# 알고리즘 생성
from sklearn.ensemble import RandomForestClassifier

# 하이퍼 파라미터 n_estimators : 결정트리 알고리즘 사용할 개수 설정, 기본값 = 100
# 알고리즘별로 다른 파라미터를 설정해주기 위해 하이퍼 파라미터 튜닝 기법 적용함
rf_clf = RandomForestClassifier(random_state = 0)

# 학습
rf_clf.fit(X_train, y_train)

# 예측
y_pred = rf_clf.predict(X_test)

# 성능평가
accuracy_score(y_test, y_pred)

"""- 정확도 상승 및 알고리즘 파라미터 튜닝"""

# 알고리즘 생성
from sklearn.model_selection import GridSearchCV

# n_jobs : 학습시 CPU 코어를 몇개 사용할지 설정(-1은 모든 코어 사용)
rf_clf = RandomForestClassifier(random_state = 0, n_jobs = -1)
param_grid = {
    # '파라미터명' : [값의 후보들]
    'n_estimators' : [100, 200], # 모델별 결정트리 알고리즘 개수
    'max_depth' : [6, 8, 10, 12], # 결정트리의 최대 깊이값 배분
    'min_samples_split' : [8, 12, 16], # 규칙노드에서 브랜치를 수행하기 위한 최소 데이터 수
    'min_samples_leaf' : [8, 16, 24], # 리프노드로 결정되는 최소 데이터 수
}
grid_cv = GridSearchCV(rf_clf, param_grid, cv = 5, n_jobs = -1)

# 학습
grid_cv.fit(X_train, y_train)

# 최적의 파라미터, 점수 확인
print(grid_cv.best_params_)
print(grid_cv.best_score_)

# 결과
# 단일 알고리즘보다 점수가 낮음

# 특성의 중요도 확인
rf_clf = RandomForestClassifier(random_state = 0)
rf_clf.fit(X_train, y_train)
print(rf_clf.feature_importances_)

# 분류의 성능을 높이기 위해서 영향력이 낮은 컬럼(특성)은 제거 또는 압축
# 그 판단을 위해서 시각화하기
import matplotlib.pyplot as plt
import seaborn as sns

tmp = pd.DataFrame(rf_clf.feature_importances_,
                   index = cancer.feature_names)
tmp = tmp.sort_values(by = 0, ascending = False)
print(tmp)
print(tmp.index)

sns.barplot(x = tmp[0], y = tmp.index)
plt.show()

"""## 부스팅(Boosting)

- 최근 많이 사용하는 알고리즘
- 원리 : 여러개의 성능이 안 좋은 분류기(알고리즘)를 순차적으로 학습/예측하고 잘못 예측한 데이터에 가중치를 부여해 오류를 개선하는 학습을 진행하는 방식
- 여러개의 알고리즘들은 각자의 방식으로 오류를 개선함

### AdaBoost
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_부스트.jpg')
# step 1 : 분류 기준 1을 기준으로 윈쪽은 + 영역, 오른쪽은 - 영역으로 간주함, - 영역에 + 가 있으면 동그라미 표기함
# step 2 : - 영역에 있는 잘못 분류된 + 는 오류라서 이 + 에 가중치를 부여함, + 가 두껍게 표기됨
# step 3 : 가중치를 받은 +를 포함해 분류 기준 2를 기준으로 왼쪽은 + 영역, 오른쪽은 - 영역, 또 오류가 있음
# step 4 : step 2 처럼 - 오류에 가중치 부여함
# 위 step 반복해서 결합하면 분류기준이 생성됨 -> 시간이 오래 걸림

# 위 그림 단순화한 그림
Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_부스트2.jpg')

"""### GBM(Gradient Boost Machine)

- 경사하강법을 가중치를 계산하는데 활용하는 기법
- 방법
  - 특정값(ex. 0.001)을 지속적으로 갱신(미세 조정)하며 예측값이 향상되었는지 확인
  - 최적의 정확도(또는 오차값을 최소화)가 나올때까지 미세조정을 계속함 -> 학습 시간이 많이 걸림
  - 이때 시간이 지날수록 값의 추이가 기울이지기 때문에 경사하강법이라고 부름
  - 딥러닝의 최적화 기법에서 등장, 여러가지 기법들이 존재
- AdaBoost와의 차이점 : 가중치 갱신
"""

from sklearn.ensemble import GradientBoostingClassifier
# 시간 측정용
import time

# 시작시간
start_time = time.time()

# GBM
gbm_clf = GradientBoostingClassifier(random_state = 0)
gbm_clf.fit(X_train, y_train)
y_pred = gbm_clf.predict(X_test)
print(accuracy_score(y_test, y_pred))

# 종료시간
time.time() - start_time

# 하이퍼 파라미터 튜닝
# learning_rate : 가중치(학습시 적용되는 학습비율), 이 비율을 미세조정하면서 정확도를 계산함
# n_estimators : 분류기에 사용할 결정트리 개수
# subsample : 분류기에 사용할 학습 데이터 샘플링 비율
# loss : 손실함수
param_grid = {
    'learning_rate' : [0.01, 0.05, 0.1],
    'n_estimators' : [100, 200],
}
gbm_clf = GradientBoostingClassifier(random_state = 0)
grid_cv = GridSearchCV(gbm_clf, param_grid, cv = 3, verbose = True)
grid_cv.fit(X_train, y_train)

# 최고의 성능은 낸 조합으로 예측
y_pred = grid_cv.best_estimator_.predict(X_test)
print(accuracy_score(y_test, y_pred))

"""### XGBoost

- 부스팅 기법 중 가장 많이 사용됨
- GBM의 단점(느린 속도, 과적합 방지책이 없음)을 개선한 방식
- 병렬 학습이 가능함 -> 학습 효율 극대화
- 하이퍼 파라미터는 GBM과 동일하며 과적합 관련 사항 추가됨
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_xgboost1.jpg')

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_xgboost2.jpg')

"""- 학습시 별도의 학습 데이터 포맷이 존재함(DMatrix 형식)
- cv : 검증 폴드에 대한 설정
- startfield : 층화 표본 추출 -> 모집합(학습 데이터 전체)들을 가지고 cv로 세트 구성할 때 서로 중복되지 않게, 각 층으로 나누어서 표본을 추출하는 방식
- early_stopping_rounds : 조기 학습 종료 -> 어느 정도의 정확도, 손실값을 설정해서 이 값에 수렴하면 학습을 조기 종료함
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_xgboost3.jpg')

# 순수 xgboost 스타일
import xgboost as xgb

# scikit-learn 스타일로 wrapping 된 클래스
from xgboost import XGBClassifier

xgb.__version__

# 데이터 준비(DMatrix)
dtrain = xgb.DMatrix(data = X_train, label = y_train)
dtest = xgb.DMatrix(data = X_test, label = y_test)

# 하이퍼 파라미터
# eta : 가중치(학습비율)
# max_depth : 트리 최대 깊이, 홀수로 지정
# min_child_weight : 하위 노드들이 가지고 있는 가중치의 합, 홀수로 지정
# gamma : 브랜치시 최소 감소값을 지정, 손실함수값(오차값)을 고려해 조정
# subsample : 각 트리에서 관측되는 데이터의 샘플링 비율, 촤소/과대적합 조정 파라미터
# colsample : 각 트리의 특성(컬럼) 샘플링 비율
# lambda : 가중치에 대한 L2 정규화, 릿지 알고리즘
# alpha : 가중치에 대한 L1 정규화, 리쏘 알고리즘

# 도구 관련 파라미터(튜닝 관련 파라미터 아님)
# eval_metric : 평가도구
## rmse(root mean square error) : 정답과 예측 사이의 오차율(작을수록 좋음)
## mse(mean square error) : 평균 제곱근 오차
## mae(mean absolute error) : 평균 절대값 오차
# seed : 난수 시드
# objective
## reg:linear - 기본값
## binary:logistic - 이진 분류(분류될 확률, 여기서는 악성으로 분류될 확률)
## multi:softmax - 다중 클래스 분류(지분율)
## multi:softprob - 다중 클래스 분류(지분 확률)

# 학습시 파라미터를 부여, 처리를 위해 세팅해주기
params = {
    'objective' : 'binary:logistic', # 예측된 결과는 2종류이므로 바이너리(이진)이고 개별 클래스에 대한 확률임
    'eval_metric' : 'logloss', # 언제 학습을 종료할 지 평가 기준, 손실함수 값을 기준으로 판단함
    'max_depth' : 3, # 트리의 깊이, 과적합 방지에 대한 옵션
    'eta' : 0.1, # 가중치를 조절하는 기준값, 이 값을 기준으로 미세조정해 가중치를 조절함
    'early_stopping_rounds' : 100 # 원하는 수준에 도달하고 더이상 값의 변화가 없으면 학습을 조기 종료함
}

evals = [(dtrain, 'train'), (dtest, 'eval')]

xgb_model = xgb.train(params = params,
                      dtrain = dtrain,
                      num_boost_round = 400,
                      evals = evals)

"""- 위 결과 해석
  - 총 400회 학습 진행
  - 가중치를 조정해서 정확도를 올림 -> 손실값이 감소하는 결과가 나옴
  - 100회에서 학습이 조기 종료되지 않음, 이 시점에서는 만족할만한 평가가 나오지 않았기 때문
"""

# 예측에 사용할 데이터 확인
print(y_test)
y_pred = xgb_model.predict(dtest)
y_pred

# y_pred를 y_test의 형태(0, 1, 0, ...)로 변환해주기
y_preds = [1 if n > 0.5 else 0 for n in y_pred]
print(y_preds)

# 성능평가, 혼동행렬 지표를 출력
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
def display_all_score(y_label, y_pred) :
  '''
  y_label : 정답
  y_pred : 예측값
  '''
  print('혼돈행렬', confusion_matrix(y_label, y_pred))
  print('정확도', accuracy_score(y_label, y_pred))
  print('정밀도', precision_score(y_label, y_pred))
  print('재현율', recall_score(y_label, y_pred))
  print('F1-score', f1_score(y_label, y_pred))
  print('roc, auc', roc_auc_score(y_label, y_pred))
  print('-' * 30)

display_all_score(y_test, y_preds)

# 데이터의 특성의 중요도 시각화
from xgboost import plot_importance

_, ax = plt.subplots(figsize = (16, 10))
plot_importance(xgb_model, ax = ax)

# scikit-learn 스타일로 진행
xgb_w = XGBClassifier(n_estimators = 400, random_state = 0)

eval_set = [(X_test, y_test)]
xgb_w.fit(X_train, y_train,
          early_stopping_rounds = 300,
          eval_set = eval_set,
          eval_metric = 'logloss')

xgb_w_pred = xgb_w.predict(X_test)
xgb_w_pred
# 래핑 스타일이라서 정답 데이터 형태(y_test)로 변환되서 나옴

display_all_score(y_test, xgb_w_pred)

"""### LightGBM

- XGBoost는 GBM 대비 속도는 개선됐지만 그래도 다른 알고리즘보다 느림(파라미터가 너무 많고 복잡함)
- 리소스를 적게 사용하고 파라미터를 간결하게 해서 학습 속도 상승
- 특징 : 균형 트리 분할에서 비대칭이라도 리프노드 중심 트리 분할에 초점
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_lightGBM.jpg')

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_lightGBM_param.jpg')
# 파라미터들

# 파이썬 스타일
import lightgbm

# scikit-learn 스타일
from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(n_estimators = 400)

eval_set = [(X_test, y_test)]
lgbm_clf.fit(X_train, y_train,
             early_stopping_rounds = 300,
             eval_metric = 'logless',
             eval_set = eval_set)
# 손실값이 xgb보다 0.01 감소했다

# 예측 및 성능평가
y_pred = lgbm_clf.predict(X_test)
display_all_score(y_test, y_pred)

"""- 결론
- 성능이 거의 유사하게 나옴
- 부스팅 기법에서 XGBoost, LightGBM은 같이 활용하는게 적절해 보임

## 스태킹(Stacking)

- 여러 모델의 예측 결과를 기반으로 메타 정보를 구축, 적재(stack)하고 이 정보를 이용해 학습, 예측을 수행함
- 예측결과가 데이터가 되는 구조
- 모델은 2단계에 걸쳐서 생성함
  - 1차 모델 : 원본 데이터를 이용해서 여러개의 알고리즘으로 예측
  - 2차 모델 : 예측 결과를 데이터로 활용해서 학습, 예측
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_스테킹1.jpg')

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_스테킹2.jpg')

"""### step 1 (1차 모델 생성)"""

# 서로 다른 여러개의 알고리즘 가져오기
from sklearn.ensemble import AdaBoostClassifier

knn_clf = KNeighborsClassifier(n_neighbors = 4)
rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 0)
dt_clf = DecisionTreeClassifier()
ab_clf = AdaBoostClassifier(n_estimators = 100)

"""### step 2 (2차 모델 생성)"""

# 회귀 계열 사용
lr_clf = LogisticRegression()

"""### step 3 (데이터 준비)"""

# 위에서 한거 사용해서 확인만
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""### step 4 (학습)"""

knn_clf.fit(X_train, y_train)
rf_clf.fit(X_train, y_train)
dt_clf.fit(X_train, y_train)
ab_clf.fit(X_train, y_train)

"""### step 5 (예측)"""

knn_pred = knn_clf.predict(X_test)
rf_pred = rf_clf.predict(X_test)
dt_pred = dt_clf.predict(X_test)
ab_pred = ab_clf.predict(X_test)

knn_pred.shape, knn_pred[:10]

"""### step 6 (예측 결과를 2차 데이터로 준비)"""

# 이 단계에서의 결과물 : (143, 4) 형태
meta_data = np.array([knn_pred, rf_pred, dt_pred, ab_pred]).T
meta_data.shape

"""### step 7 (2차 모델 학습)"""

# y_test로 학습해야 함
lr_clf.fit(meta_data, y_test)

"""### step 8 (2차 모델 예측)"""

# 정확도를 위해서 한번도 접하지 않은 데이터로 예측해야 하는데 현재 준비된 모든 데이터를 모두 사용함
# 여기서는 재사용하겠음
accuracy_score(y_test, lr_clf.predict(meta_data))

"""### cv를 적용한 학습 과정 형태"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_스테킹3.jpg')

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_스테킹4.jpg')

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_스테킹5.jpg')

Image('/content/drive/MyDrive/머신러닝/0404_res/today/분류_앙상블3_스테킹6.jpg')

"""- 위 그림 요약
1. 데이터를 학습용, 검증용 데이터로 분리
2. 위에서 분리한 학습용 데이터를 구분(물리적 분리 아님)해서 1차 모델에서 학습용, 검증용으로 사용하고 결과를 적재함
3. 1번의 검증용 데이터와 2번의 1차 모델에서 적재한 데이터를 활용해서 2차 모델 진행

#### 실습
"""

knn_clf = KNeighborsClassifier(n_neighbors = 4)
rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 0)
dt_clf = DecisionTreeClassifier()
ab_clf = AdaBoostClassifier(n_estimators = 100)

from sklearn.model_selection import KFold

# make_stacking_data(알고리즘, 학습용 데이터, 학습용 정답 데이터, 검증용 데이터)
# 리턴값
## 검증시 예측의 결과물을 담은 데이터 -> 2차 모델의 학습용
## 검증용 데이터를 널어서 예측한 데이터 -> 2차 모델의 검증용
def make_stacking_data(clf, X_train, y_train, X_test, n_folds = 7) :
  '''
  지도학습 > 분류 > 앙상블 > 스태킹 > 1차 모델을 이용한 메타 데이터 생성 함수

  매개변수
  - clf : 알고리즘
  - X_train : 1차 학습 데이터
  - y_train : 1차 정답 데이터
  - X_test : 1차 검증 데이터
  - n_folds : cv값, 기본값은 7세트

  반환값
  - train_fold_valid_pred : 모델 학습시 검증폴드를 이용하여 예측을 수행하고 그 결과를 모은 데이터
  - test_pred_avg : 모델 학습 종료 후 한번도 접하지 않은 데이터를 넣어서 예측한 결과의 평균

  ex) 데이터 10개, cv = 5일 때 1세트의 데이터 수 : 2개
  '''
  # cv를 처리하기 위해서 폴드 객체 생성
  kf = KFold(n_splits = n_folds, shuffle = False)

  # 초기 예측 결과를 담은 변수(배열)는 0으로 초기화해서 준비
  ## cv를 이용한 학습 특성상 각 데이터는 1번만 검증되어 예측값이 생성됨
  train_fold_valid_pred = np.zeros((X_train.shape[0], 1))
  ## cv를 통해서 만들어진 세트수만큼 검증 데이터를 사용하여서 예측하기 때문에
  ## 검증 예측을 수행하는 횟수는 n_folds와 동일함
  test_pred_avg = np.zeros((X_test.shape[0], n_folds))

  for idx, (train_idx, valid_idx) in enumerate(kf.split(X_train)) :
    # 데이터를 물리적으로 분리하는 것이 아니라 데이터 중에서 7번 반복하는 동안
    # 학습에 참여할 데이터의 인덱스와 검증에 참여할 데이터의 인덱스를 구분해주고
    # 모든 데이터는 반드시 1번씩은 검증용으로 사용됨

    # 1. 각 fold 기준으로 학습용 데이터, 검증용 데이터, 학습용 정답 데이터 추출
    X_trs = X_train[train_idx] # 주어진 인덱스에 해당되는 학습 데이터만 추출 -> 365개
    y_trs = y_train[train_idx] # 주아잔 안덱스에 해당하는 정답 데이터만 추출 -> 365개
    X_val = X_train[valid_idx] # 주어진 인덱스에 해당하는 검증 데이터만 추출 -> 61게(또는 60개)
    # 2. 학습
    clf.fit(X_trs, y_trs)
    # 3. 예측 : 검증용 데이터 사용
    tmp = clf.predict(X_val)
    # 4. 검증용 데이터로 예측된 데이터를 적재
    # 2차원 데이터를 2차원 위치에 대입
    train_fold_valid_pred[valid_idx, : ] = tmp.reshape(-1, 1)
    # 5. 검증용 데이터(1번에서 분리한 데이터말고 매개변수로 받은 X_test)로 예측된 데이터를 적재
    # 1차원 데이터를 1차원 위치에 대입
    test_pred_avg[ : , idx] = clf.predict(X_test)
  
  # for문 밖에서 test_pred_avg를 평균
  # test_pred_avg의 현재 형태 (143, 7) -> 평균처리 -> (143, 1)
  test_pred_avg = np.mean(test_pred_avg, axis = 1).reshape(-1, 1)

  return train_fold_valid_pred, test_pred_avg

# 알고리즘별 적용
knn_train, knn_test = make_stacking_data(knn_clf, X_train, y_train, X_test)
rf_train, rf_test = make_stacking_data(rf_clf, X_train, y_train, X_test)
dt_train, dt_test = make_stacking_data(dt_clf, X_train, y_train, X_test)
ab_train, ab_test = make_stacking_data(ab_clf, X_train, y_train, X_test)

# 위에서 알고리즘별로 생성된 데이터를 통합
stacking_X_train = np.concatenate((knn_train, rf_train, dt_train, ab_train), axis = 1)
stacking_X_test = np.concatenate((knn_test, rf_test, dt_test, ab_test), axis = 1)
# stacking_X_test는 평균낸 값이라서 0, 1이 아닌 값이 존재할 수 있음

# 2차 모델 학습
# 알고리즘 생성
lr_clf = LogisticRegression()

# 학습
lr_clf.fit(stacking_X_train, y_train)

# 예측 및 평가
accuracy_score(y_test, lr_clf.predict(stacking_X_test))

"""# SVM

### 개념

- 가장 넓은 여백을 가지게끔 클래스로 구분할 수 있게 경계선을 찾음
- 2개의 클래스를 확실하게 분류할 수 없다면 가능한 최적의 경계선을 찾음
- 이미지 분류에서 많이 사용함
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/ml-서포트백터머신.png')

"""# KNN

### 개념

- 클러스터링(군집)에서 많이 사용함
"""

Image('/content/drive/MyDrive/머신러닝/0404_res/today/ml-최근접이웃.png')