# -*- coding: utf-8 -*-
"""2.머신러닝_기초_실습(Flask_활용_웹서비스).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PwcKVdmRD4FEzTj510KeKEYuG489TVKI

# 연구 목표 수립

- 유사 서비스 구현
  - 파파고, 구글 번역
  - **언어감지**를 머신러닝으로 구현해 웹으로 서비스해보기
- 목표
  - 머신러닝으로 만들어진 모델이 어떻게 적용되는지 이해
  - 앞선 과정과 머신러닝에서 산출까지 어떻게 연결되는지 이해
- 조건
  - 머신러닝을 배우지 않았기 때문에 필요한 것 외에는 전부 가정하고 진행함
  - 언어 감지 영역
    - 알파벳을 사용하는 언어권 대상
    - A to Z (26개) 문자 대상
    - 논문 참고
      - 알파벳을 문자로 사용하는 국가별로 언어체계의 A to Z 까지의 사용 빈도가 다르다(관련 논문)
      - 이 주장(알파벳 사용 빈도가 다르다)을 기반으로 프로젝트 진행
      - 문자 26개, 정답 1개
      - shape = (n, 26 + 1)
      - **A to Z 까지 특정 말뭉치상에 등장한 빈도수 전처리 필요**
      - 예측 결과
        - 이 말뭉치는 영어입니다. / 이 말뭉치는 프랑스어입니다.
        - 위와 같은 표현은 지도학습(정답이 있기 때문)
        - 정답에 카테고리화가 되어 있어서 분류의 문제임
- 산출물
  - 웹 서비스 구현
    - 페이지 1장
    - 가장 간단하게 flask 를 통해 구현
      - 장점 : 마이크로 에디션, 경량, 높은 자유도, 모듈이식이 간단
"""

from IPython.display import Image

# 산출물 예시
Image('/content/drive/MyDrive/머신러닝/res/산출물.png')

"""# 데이터 수집

- 실전 방식
  - 여기서는 받은 test, train 데이터 활용

- 수업 방식
  - 위키 피디아에서 데이터 수집 -> 전처리 -> 정규화 -> 정제
  - 데이터 형태 = (n, 26 + 1)
  - 26개 독립변수는 수치(빈도수)로, 1개의 종속변수는 국가코드로 표현(en, fr,...)
  - 아래 링크 데이터를 기준으로 말뭉치를 가져와서 데이터화
    - https://en.wikipedia.org/wiki/BTS
    - 타겟 사이트 점검 결과 level 3 웹 크롤링 기술만으로 가능
      - BS4를 사용해 필요한 만큼 추출(텍스트만 추출)
      - BS4 -> html 텍스트 -> 파싱 -> Dom tree -> html을 탐색해서 원문만 추출(css selector or xpaht)
      - **css selector 사용**
        - **#mw-content-text p**
        - 이 요소 하위에 있는 텍스트만 추출

- 제공된 데이터 활용(최종)

## 모듈 가져오기
"""

# 있는지 확인
!pip list

import urllib.request as req
from bs4 import BeautifulSoup

"""## 사이트 접속"""

target_site = 'https://en.wikipedia.org/wiki/BTS'
res = req.urlopen(target_site)
res

"""## 파싱

- 파싱 : html을 DOM(Document Object Model) 모델로 로드하는 작업
- html의 마크업 언어를 객체화해서 조작, 탐색하게끔 제공해해주는 기술
- 파서(parser)를 활용
  - 'html5lib' : 느리지만 대용량 가능하고, 정확하게 처리함
"""

soup = BeautifulSoup(res, 'html5lib')
soup

"""## 데이터 추출"""

# 추출
# select : 해당 css selector와 일치하는 모든 요소를 가져옴 -> list
ps = soup.select('#mw-content-text p', )
len(ps)

# text : 태그는 지우고 텍스트만 추출하기
tmp = list()

for p in ps :
  tmp.append(p.text)

# 리스트 내포형 : 훨씬 간단함
# tmp = [p.text for p in ps]

# 확인
tmp

# 알파벳만 남아야 하는데 데이터가 지저분함

"""- 데이터 수집 단계에서 데이터 전처리 단계로 산출물을 넘길 때, 말뭉치(하나의 문자열 형태)로 넘기기"""

# 리스트 내의 문자열들을 하나의 문자열로 전달하기
# 구분자를 넣고 한 개의 문자열로 통합하기 -> 문자열.join(연속형 데이터)
tmps = ''.join(tmp)
tmps

"""- 미리 전처리 해주기
- A to Z 가 아닌 문자는 모두 제거하기(정규식 활용)
"""

import re

# 정규식
# [] : 문자(글자) 한 개
# a-z : 알파벳 a ~ z 까지 26개 중 한 개
# [a-z] : a ~ z 중 한 개
# [a-zA-Z] : 대소문자 52개 중 아무거나 한 개
# * : 0 ~ 무한
# [^] : [^...] 대괄호 안에 있는 글자는 제외

# 패턴 정하기
pattern = '[^a-zA-Z]*'

# re 라이브러리 사용해서 패턴 적용
p = re.compile(pattern)

# 알파벳이 아닌 문자들을 찾아서 빈 칸으로 대체함
tmp = p.sub('', tmps)

# 일반적으로 영어 알파벳을 자연어 처리할 때는 보통 소문자로 바꿈
tmp = tmp.lower()

# 확인하기
tmp

"""# 데이터 준비 / 전처리 / 정제

- 현재 상황
  - 데이터 수집 파트로부터 train/*.txt 파일 20개를 제공받았음(훈련용)
  - 데이터 수집 파트로부터 test/*.txt 파일 20개를 제공받았음(검증용)
    - 검증용 데이터는 훈련시 만나보지 못 한 데이터여야 함
  - 일반적으로 훈련용, 검증용 데이터는 75:25 비율로 진행함
    - 훈련용 데이터 75 안에서 또 50:25로 나눠서 훈련하는 중 검증을 하기도 함

- 산출물
  - (n, 26 + 1) 형태의 csv 혹은 json 등의 포맷
  - 우리는 json 형태로 저장함
  - 형태
    - json 형태
    - 독립변수는 2차원, 종속변수는 1차원임
    - A to Z의 빈도수는 각 훈련용 데이터(총 20개) 별로 계산해 배열에 넣고 이를 또 배열로 묶어서 freqs라는 키값에 저장
    - 훈련옹 데이터 총 20개의 각각 언어 코드는 labels라는 카값에 저장


```
{
  "freqs" : [[
    a의 빈도수, b의 빈도수, ..., z의 빈도수, '해당 언어'
  ], [], ...],
  "labels" : [
    "en", "fr", ...
  ]
}
```

## 파일 경로 설정
- 훈련용 : /content/drive/MyDrive/머신러닝/res/train/*.txt
- 검증용 : /content/drive/MyDrive/머신러닝/res/test/*.txt
- 라이브러리 : glob
"""

import glob

files = glob.glob('/content/drive/MyDrive/머신러닝/res/train/*.txt')
files

"""## 파일 불러오기 및 전처리

### 정답(종속변수) 추출

- '/content/drive/MyDrive/머신러닝/res/train/**en**-1.txt'
- 파일명 앞에서 두 글자가 정답임
  - 파일명 추출 -> 정답 추출 -> 전체 files에 적용
"""

# 문자열로 인식하고 추출하는 방법
print(files[0].split('/')[7][:2])

# 경로로 인식하고 추출하는 방법 : os.path.basename()
# 이 방법으로 진행함
import os

print(os.path.basename(files[0]), os.path.basename(files[0])[:2])

naCode = os.path.basename(files[0])[:2]
print(naCode)

# 정규식으로 추출하는 방법
# ^이 대괄호 밖에 있으면 그 문자로 시작하는 것을 찾음
# 아래 해석 : 알파벳 소문자 2개로 시작하는 것을 찾음
p = re.compile('^[a-z]{2}')
fName = os.path.basename(files[0])
print(fName)
print(p.match(fName))
p.match(fName).group()

"""### 빈도 추출

- 파일 1개당 산출물 : [a의 빈도수, b의 빈도수, ..., z의 빈도수]
"""

# 파일읽기 : I/O -> 오류 동반(파일, 네트워크, 데이터 베이스) -> 예외 처리(try - except)
# with문 사용 권장 : close 해줘야 하는데 자동 닫기 제공함

with open(files[0]) as f :
  # 1. 파일을 읽고 소문자로 변환
  text = f.read().lower()
  print(text[:100])
  # 2. 알파벳 소문자만 남기고 모두 제거 혹은 전처리 과정 수행
  p = re.compile('[^a-z]*')
  text = p.sub('', text)
  print(text[:100])
  # 3. 한 글자씩 뽑아서 count
  for ch in text :
    print(ch)
    break
  # 4. 리스트에 담음
  # a : 0번 자리, b : 1번 자리, ..., z : 25번 자리
  # 자리 값은 (개별 알파벳의 아스키 값) - (a의 아스키 값)
  print(ord('a'))
  pass

# 실습
with open(files[0]) as f :
  text = f.read().lower()

  p = re.compile('[^a-z]*')
  text = p.sub('', text)

  # 미리 알파벳 26개 개수가 들어갈 공간 만들어놓기
  list_count = [0 for i in range(26)]

  # 반복문 밖에다가 상수 스타일로 사용
  STD_INDEX = ord('a')

  for ch in text :
    index = ord(ch) - STD_INDEX
    list_count[index] += 1

  print(list_count)
  pass

# 정규화해보기
# 데이터를 보니 편차가 너무 커서 훈련시 효과가 좋지는 않을 듯
# 값의 범위를 0 ~ 1 사이로 정규화해서 훈련시 정확도가 높아지게 조정해보기
# 학습 후 최적화 단계에서 해도 되는데 그냥 지금 함
# 개별 빈도수 / 전체 빈도수 => 각 알파벳이 가지는 비율(총합 : 1.0)
# => 영향력을 의미 => 각 국가의 언어의 특성으로 작용
# 함수 : softmax
# 그런데 함수 없이 직접 비슷하게 구현해보자

# list_count의 개별 값들을 개별 값 / 전체 값으로 계산하여 대체하기
# 미리 전체 값을 계산해놓기
sum_count = sum(list_count)

# lambda는 파이썬에서 가장 빠름, 대신 한 줄로 표현해야 함
# x는 개별값, 개별 값 / 전체 값으로 계산으로 계산해서 대체함
try :
  cnt_norms = list(map(lambda x : x / sum_count, list_count))
except :
  pass

# 확인하기
print(cnt_norms)
print(sum(cnt_norms))

# 정규화한 값이 1.0이 아닐 수도 있어서 정확하게 할 때는 함수로 계산하자

# 위의 정답 추출, 빈도 추출 작업을 for문으로 한번에 해보기
freqs = list()
labels = list()

for file in files :
  # 정답 추출
  label = os.path.basename(file)[:2]
  labels.append(label)

  # 빈도 추출
  with open(file) as f :
    text = f.read().lower()

    p = re.compile('[^a-z]*')
    text = p.sub('', text)

    list_count = list(range(26))

    STD_INDEX = ord('a')

    for ch in text :
      index = ord(ch) - STD_INDEX
      list_count[index] += 1

    sum_count = sum(list_count)

    cnt_norms = list(map(lambda x : x / sum_count, list_count))

    freqs.append(cnt_norms)

    pass

# 확인
print(labels)
print(freqs)

"""- 텍스트를 넣어서 빈도데이터로 만드는 부분은 falsk로 웹 구현시 사용자 입력값을 동일하게 처리해야 하는 부분이 존재함
- 이 부분은 함수로 만들어서 모듈화 필요
"""

# 함수화
# 단, 웹에서 사용시에는 파일은 없음
# 기본값으로 train 넣어놓음
import glob
import os
import re

def makeData(path = 'train') :
  # 파일 불러오는 아래 한 줄은 flask에서 제외됨
  files = glob.glob(f'/content/drive/MyDrive/머신러닝/res/{path}/*.txt')
  freqs = list()
  labels = list()
  for file in files :
    label = os.path.basename(file)[:2]
    labels.append(label)
    with open(file) as f :
      text = f.read().lower()
      p = re.compile('[^a-z]*')
      text = p.sub('', text)
      list_count = [0 for i in range(26)]
      STD_INDEX = ord('a')
      for ch in text :
        index = ord(ch) - STD_INDEX
        list_count[index] += 1
      sum_count = sum(list_count)
      cnt_norms = list(map(lambda x : x / sum_count, list_count))
      freqs.append(cnt_norms)

  return {
    "freqs" : freqs,
    "labels" : labels
  }

  pass

train_Data = makeData()
test_Data = makeData('test')

print(train_Data)

"""## 데이터 적제(저장)

- 산출물 :  train.json, test.json
- 자료구조를 유지하면서 파일에 기록되는 형태
  - json 라이브러리 사용
"""

import json

# 파일 저장
with open('train.json', 'w') as f :
  json.dump(train_Data, f)

with open('test.json', 'w') as f :
  json.dump(test_Data, f)

"""# 데이터 분석

## 모듈 가져오기

- pandas 이용
  - 데이터 프레임 구축
    - *.json 파일을 읽어서 df를 구성
  - 시각화를 통해 언어별로 a - z의 빈도가 다름을 확인
  - 나름 이 부분에 대한 주장이 근거가 있음을 확인
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json

# %matplotlib inline

# 주피터에서 작업한 모듈의 버전과 sw 레벨에서 사용한 모듈의 버전이 일치해야 함
np.__version__, pd.__version__

"""## 데이터 프레임 구성"""

# json 읽어오기
with open('/content/train.json') as f :
  tmp = json.load(f)

# df 생성 (a - z 빈도를 담고 있는 데이터를 df로 바꿈)
df_freqs = pd.DataFrame(tmp['freqs'])
df_freqs

# 컬럼명 변경
# a - z 까지의 소문자들
from string import ascii_lowercase

df_freqs.columns = list(ascii_lowercase)
df_freqs

df_freqs.shape
# 현재 형태 : (20, 26)
# 아직 정답 컬럼이 없어서 join이나 concat, merge로 합치기

# 정답 컬럼 불러와서 데이터 프레임으로 만들기
df_labels = pd.DataFrame(tmp['labels'])
df_labels.columns = ['label']

df_tmp = pd.concat([df_freqs, df_labels], axis = 1)
print(df_tmp.shape)
df_tmp

"""## 시각화를 통한 데이터 확인

- 언어별 알파벳의 빈도 확인
"""

# 컬럼 중 label 컬럼의 값에는 중복 값이 있음
df_tmp['label'].unique()

# 머신러닝 > 지도학습 > 분류의 종속변수는 카테고리화 되어 있어서 카운트가 가능함
# 피벗 테이블이 되기 좋은 구조임
# aggfunc에 다른 값을 안 넣어서 기본값인 평균으로 데이터가 통합됨
df_tmp_pv = df_tmp.pivot_table(index = df_tmp['label'])
df_tmp_pv

# 재구조화
df_tmp_pv.T

# 시각화
# 스타일 정의
plt.style.use('ggplot')

# 그려보기
# T를 써서 재구조화(축 변경)
# x축 : a - z
# y축 : 빈도값
# 컬럼별 차트 생성
df_tmp_pv.T.plot(kind = 'bar',
               subplots = True,
               figsize = (15, 10),
               ylim = (0, 0.25))

# 보여줘
plt.show()

# 위의 차트를 보고 개별적인 a - z의 차이를 구분하기는 힘들기 때문에 각 문자별 빈도분포를 그려기
# 한 개의 차트에 a라는 문자의 빈도를 각각 언어별로 그리기 -> 히스토그램

# 미리 국가 리스트 만들어 놓기
na_codes = df_tmp['label'].unique()

# 알파벳별로 추출해서
for ch in ascii_lowercase :
# 해당 알파벳별로 그래프 그리기
  for na_code in na_codes :
    # 전체 데이터 df_tmp에서
    # 컬럼명이 ch이고 label 컬럼값이 na_code인 데이터 추출
    tmp = df_tmp[(df_tmp['label'] == na_code)][ch]

    # 그리기
    tmp.plot(kind = 'hist',
             alpha = 0.5,
             label = na_code)
  # 범례
  plt.legend()
  # 제목
  plt.suptitle(f'{ch}\'s freqs histogram')

  # 출력
  plt.show()
  break

# 선형 그래프 그려보기
df_tmp_pv.T.plot(kind = 'line',
                 figsize = (15, 5))

"""# 모델 구축

- 머신러닝, 딥러닝 등 예측 모델을 구축
- 순서
  - 알고리즘 선정
  - 훈련용 데이터, 검증용 데이터 준비
  - 훈련 - fit
  - 예측
  - 성능 평가
  - 최적화 --> 다시 훈련으로 (원하는 결과가 나올 때까지)
  - 모델 덤프 : 산출물로 사용할 수 있는 형태로 만들기

## 머신러닝 사용 버전 확인 및 동기화

- 주피터 환경에서 사용한 모듈(라이브러리)의 버전과 산출물 제작에 사용하는 모듈의 버전이 일치해야 함
- 최종 파일 : 
  - requirements.txt
  - 생성법 : pip freeze > requirements.txt
- 서비스 환경
  - 1. 가상환경 구축
  - 2. requirements.txt를 이용해 패키지 설치
"""

# 현재 코랩 scikit-learn : 0.18.3
!pip list

# 현재 pc의 scikit-learn : 0.24.2

# 코랩을 맞춰주기
!pip install scikit-learn==0.24.2

import sklearn

# scikit 모듈 버전 확인하기
sklearn.__version__

# 여기서는 플로우가 중심이라서 알고리즘 선정에 특별한 사유는 없음
# 머신러닝 선택시트에 의해 svm 모듈의 svc를 사용함
from sklearn import svm

# 분류기 : classfication
clf = svm.SVC()

# 여기서 알고리즘의 파라미터 등은 기본값으로 사용함
# 나중에 이 파라미터들을 가용범위에 여러 후보를 두고 하이퍼 파라미터 튜닝을 진행함
clf

"""## 알고리즘 선정

- 알고리즘 고르기
  - 공식 가이드 : 머신러닝 선택시트 활용
  - 유사한 경험이 있다면 경험적으로 선택
  - 유행하는 주류 알고리즘 사용 : 앙상블, XG_Boost 등
  - 단기간 대회(해커톤) 에서는 기계적으로 교차검증 + 파이프라인 구축해서 상위권 알고리즘으로 최적화 진행
  - 연구소에서 제작한 알고리즘, git에 공개된 비주류 알고리즘
  - 알고리즘 새로 만들기
    - C / C++ 와 cython 필수

- 여기서는 주류 알고리즘 위주로 체크

## 훈련용, 검증용 데이터 준비

- 추천 비율 : 75 : 25 (훈련용 : 검증용)
  - 75 중에서도 50 : 25 (훈련용 : 검증용)로 나누기도 함
"""

# 위에서 훈련시킨 데이터 확인
print(train_Data)

# x값, 독립변수, 특성, feature => 여기서는 2차원 리스트
X = train_Data['freqs']
# y값, 종속변수, 클래스, 정답, label => 여기서는 1차원 리스트
y = train_Data['labels']

"""## 훈련 - fit

- 데이터를 알고리즘에 주입해서 특정 목적을 달성하도록 **반복적으로 훈련**하는 과정

- 기법
  - 훈련된 모델이 서비스에 실시간 반영되는지 확인하는 기법
    - 오프라인 학습 : 시스템 셧다운 후 반영
    - 온라인 : 실시간 반영
  - 데이터 전체를 학습시 모두 사용하는 기법
    - 한번에 다 밀어넣음 : 배치 학습
    - 나눠서 밀어넣음 : 미니 배치 학습
    - 학습시 가용 가능한 메모리를 보고 설계하기
"""

# 위의 데이터 넣어서 훈련시키기
clf.fit(X, y)

"""## 예측

- **한번도 접하지 않은 데이터(훈련시 만난적 없는 데이터)**를 학습된 모델(알고리즘, 여기서는 분류기)에 넣어서 정답을 예측한다
"""

test_X = test_Data['freqs']

pred_y = clf.predict(test_X)

pred_y

"""## 성능 평가

- 분류
  - 혼동행렬(오차행렬) 기반 : 정확도, 정밀도, F1-score, aoc, roc 등 기준갑
- 회귀
  - 손실함수 값, ...
"""

# 현재 주제는 분류의 문제이므로 혼동행렬(오차행렬) 기반 평가 지표롤 성능을 평가함
# 평가 모듈 불러오기
from sklearn import metrics

# metrics.accuracy_score(실제 정답, 예측값)
metrics.accuracy_score(test_Data['labels'], pred_y)

# 1.0에 가까워야 좋음, 1.0은 안 좋음

# 혼동행렬 기반 정밀도, 재현율, fl-score, 매크로 평균이 레포트 됨
print(metrics.classification_report(test_Data['labels'], pred_y))

"""## 최적화

- 데이터가 부족한가?
- 하이퍼 파라미터 튜닝
- 교차 검증(알고리즘 조합, 다양성 조합, 시퀀스 조합)
- **파이프라인 구축**
  - 하이퍼 파라미터, 알고리즘 조합, 전처리기 등

##  모델 덤프

- 산출물에서 사용가능하게 형태를 조절함
- 알고리즘 구조화로 학습된 조정값들이 유지되게 저장
- 일반적으로 파일 형태
"""

# 딥러닝 엔진에는 기능에 내장되어 있으나 별도의 써드파티 모듈을 사용하기도 함
import joblib

# joblib.dump(분류기, '이름.확장자')
# 확장자는 어떤 것을 사용해도 상관없지만
# 특정 모듈을 사용하면 그 확장자로 정하거나 자체 포멧 형태로 정해도 됨
joblib.dump(clf, 'lang_clf.model')

# 분류의 문제라서 정답 목록도 저장하기
label_dict = {
    'en' :'영어',
    'fr' :'프랑스어',
    'id' :'인도네시아어',
    'tl' :'타갈리아어'
}

joblib.dump(label_dict, 'lang_label')

"""## 최종 산출물

- 모델 파일 : lang_clf.model
- 정답 파일 : lang_label

# 시스템 통합, 산출물, 보고서

- flask 기반으로 1개 페이지 서비스 제작
- UI는 파파고처럼 구성
  - 디자인은 미고려, 유사하면 마무리
- 주피터에서는 안 되서 vs code에서 진행
"""

