# -*- coding: utf-8 -*-
"""6.머신러닝_비지도학습_군집.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19gYo92SGXhtoMRTec71McSmtZUwd4mY3

# 군집화(Clustering)

- 목적 : 정답이 없는 데이터(이 데이터가 무엇을 지향하는지 알 수 없음)를 알고리즘으로 군집을 만들어서 새로운 특징점 혹은 분류라는 지점을 생성하는 기법
- 목표 : 분류 알고리즘에 이 데이터를 적용하기 위해 전처리 과정을 통해 카테고리화 한 컬럼을 제공

# k-Mean (거리기반)

- 일반적으로 많이 사용함
- 군집의 중심점(정답)이라는 임시값을 선택하고 그 중심점을 기준으로 가까운 거리에 있는 데이터(포인트)들을 선택하는 기법
"""

from IPython.display import Image

Image('/content/drive/MyDrive/머신러닝/0406_res/k-mean_0.png')
# 아래 그림 설명
# 0. 최초 알고리즘 생성할 때 군집 중심점을 몇 개 만들지 결정(아래는 2개)
# 1.중심점 생성
## 랜덤으로 설정하면  운이 나쁘면 시간이 오래 걸림
## 데이터가 밀집된 곳에 평균적으로 중심점으로 하면 상대적으로 빠름
# 2. 중심점에 가까운 데이터들은 이 중심점의 멤버로 소속됨
# 3. 각 중심점을 모든 멤버들간의 평균 지점으로 이동시킴
# 4. 2 - 3번을 반복시킴
# 5. 중심점이 움직일 수 있는 횟수의 제약에 도달해서 더이상 움직이지 않으면 군집화 종료

"""- 장점 : 쉽고 알고리즘도 간단해서 가장 많이 활용됨
- 단점
  - 데이터가 많다면 군집의 정확도가 떨어짐 -> PCA 등을 활용해 압축 처리 필요
  - 위의 2 - 3번이 지속적으로 발생하면 군집화 시간이 많이 걸림
- 중심점의 개수가 애매하면 후보군을 잡고 군집화 후 평가 및 결정

## 실습
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.preprocessing import StandardScaler, scale
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
# %matplotlib inline

# 데이터 불러오기
iris = load_iris()
iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)
iris_df.head(2)

# 알고리즘 생성
# 하이퍼 파라미터들
# n_cluster : 군집(군집 중심점)의 개수(기본값은 8)
# init : 최초 중심점을 잡는 방법
## "random" : 랜덤 위치
## "k-means++"
### 1. 데이터 중 무작위로 1개 선정해 중심점으로 사용
### 2. 나머지 데이터는 중심점을 가반으로 거리 계산
### 3. 계산한 거리가 가장 먼 데이터를 두번째 중심점으로 사용
### 4. 위 단계를 반복해 군집 중심점의 개수가 나올때까지 반복
### 5. 초기 중심점 선정 완료 -> 모든 중심점은 서로 가장 멀리 떨어져 있음
# max_iter : 중심점을 선정하는 최대 횟수(기본값을 300)

kmeans = KMeans(n_clusters = 3, init = "k-means++", max_iter = 300, random_state = 0)

# 학습을 통해 군집 완성 : 군집은 학습시 데이터 다 넣음
kmeans.fit(iris_df)

# 군집의 결과 확인
# 0, 1, 2는 군집, 분류하며 임의로 부여한 번호로 정답과는 상관없을 수 있음 
print(kmeans.labels_)

# 실제 정답
# 정답과 비교해보면 51번부터는 데이터에 잡음이 보임
print(iris.target)

# 예측 : 군집 모델의 예측은 그 결과가 답이 아니라 군집화한 결과일 뿐임
# 학습시 사용한 데이터 활용(원래는 접하지 못한 데이터 써야함)
kmeans.predict(iris_df)

# 군집 중심점 정보 확인
# 중심점(정답)은 3개, 각 점은 4개의 특성(컬럼)으로 구성되어 있음
print(kmeans.cluster_centers_)
print(kmeans.cluster_centers_.shape)

# 군집이 실제 정답과 잘 매칭되는지 체크(정답이 있는 경우에만 가능)
iris_df['target']  = iris.target
iris_df['cluster'] = kmeans.labels_
iris_df.head(2)

# 실제 정답(target) 대비 군집(cluster) 카운트해서 비교
iris_df.groupby(['target', 'cluster'])[['target', 'cluster']].count()

# 0번 품종은 정답과 군집이 명확하게 잘 나눠짐
# 1, 2번 품종은 만이 겹쳐져 있음 -> 컬럼을 줄여서 진행 -> 차원 축소
# 차원축소 : PCA, LDA, SVD, NMF

"""## 차원축소 사용 (PCA)

- 차원 축소가 효율적으로 작동하려면 분류에 영향을 미치지 않는 중요하지 않은 컬럼은 제거 후 진행하는게 더 좋음
- 차원 축소(컬럼을 제거하거나 압축)가 군집의 성능을 높이지는 않음
"""

from sklearn.decomposition import PCA

# 4개의 컴포넌트(특성, 컬럼)을 2개로 압축(차원축소)
pca = PCA( n_components=2 )
pca

iris_pca = pca.fit_transform( iris.data)
print(iris_pca.shape)
print(iris_pca[:2])

# df에 추가
iris_df['pca_1'] = iris_pca[:, 0]
iris_df['pca_2'] = iris_pca[:, 1]
iris_df.head()

# 산점도 그려보기

# 군집을 기준으로 산포도 그리기
cluster_cnt = iris_df['cluster'].unique()
# 마커 모양
markers = ['o', 'x', 's']
# 군집이 3개라서 3번 반복함
for c_idx in cluster_cnt :
  # 위의 군집값을 가져와서 조건이 일치하는 것만 추출
  tmp_df = iris_df[(iris_df['cluster'] == c_idx)]
  plt.scatter(data = tmp_df, x = 'pca_1', y = 'pca_2', marker = markers[c_idx])

plt.show()

kmeans2 = KMeans(n_clusters = 3, init = "k-means++", max_iter = 300, random_state = 0)
iris_df2 = iris_df.iloc[:, -2:]
print(iris_df2.head(2))

kmeans2.fit(iris_df2)

iris_df2['target'] = iris.target
iris_df2['cluster'] = kmeans2.labels_
print(iris_df2.head(2))

iris_df2.groupby(['target', 'cluster'])[['pca_1']].count()

# 차원을 축소해도 결과는 비슷하게 나왔네

"""## 군집평가

- 실제 비지도 학습에서는 정답이 없음
- 따라서 군집이 잘 되었는지 평가 지표와 방식이 필요함

### 실루엣 분석

- 군집 사이의 거리가 효과적인지 판단
- 평가 기준
  - 군집의 데이터들은 잘 모여져 있는지
  - 군집들간의 거리는 잘 떨어져 있는지
- 군집이 잘 되어있다면 군집 내 비슷한 여유 공간이 형성되어 있음
- 방법 : 실루엣 계수
  - 개별 데이터는 실루엣 계수 값이 존재
  - 군집별로도 실루엣 계수 값이 존재
"""

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_실루엣_1.png')
# 아래 그림 해석

# 현재 군집은 3개
# 군집 A를 기준으로 보면
## 1, 2, 3, 4번 포인트(데이터)가 존재함, 1번 포인트는 a1으로 표현
## 1번 포인트를 기준으로 같은 군집 내 다른 포인트들간의 거리는 a12, a13, a14로 표현
## ai = a12 + a13 + a14 + ... / 총 개수 => 평균값 계산

# 군집 B를 보면
## 5, 6, 7, 8번 포인트가 존재함
## 군집 A의 1번 포인트 기준 다른 군집의 포인트들간의 거리는 b15, b16, b17, b18로 표현
## bi = b15 + b16 + b17 + b18 + ... / 총 개수 => 평균값 계산

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_실루엣_2.png')
# 실루엣 계수 계산식
# i : 특정 데이터 포인트
# s(i) : 특정 데이터 포인트의 실루엣 계수
# a(i) : 같은 군집 내 데이터 포인트 간의 평균 거리
# b(i) : 같은 군집에서 다른 군집의 데이터 포인트 포인트 간의 평균 거리
# b(i) - a(i) : 특정 데이터 포인터 기준 군집 간의 거리
# b(i) - a(i) / max(a(i), b(i)) : 군집 간의 거리 / 거리들 중 최대값

# 해석 : -1, 0, 1의 값이 나옴
## 1에 가까움 : 두 군집의 거리가 떨어져 있음 -> 군집이 잘 되었음
## 0에 가까움 : 두 군집의 거리가 가까움 -> 재작업의 필요성이 있음
## -1에 가까움 : 두 군집이 겹침 -> 재작업

"""### 실루엣 계수를 이용한 평가

- 0 ~ 1 사이가 되도록 해야함
- 1에 가까우면 군집이 잘 됐음
- 전체 실루엣 계수의 평균값과 군집별 평균값의 차이(편차)가 적을수록 좋음

### 시각화
"""

from sklearn.metrics import silhouette_score, silhouette_samples

iris_df = iris_df.iloc[:, :4]
iris_df.head(1)

kmeans = KMeans(n_clusters = 3, init = "k-means++", max_iter = 300, random_state = 0)
kmeans.fit(iris_df)

iris_df['cluster'] = kmeans.labels_
iris_df.head(1)

# 각 데이터 포인트마다 실루엣 계수 계산
data_per_silhouettes = silhouette_samples(iris.data, iris_df['cluster'])
print(data_per_silhouettes.shape)
data_per_silhouettes[:5], data_per_silhouettes[-5:]

# 세토사로 추정되는 종은 실루엣 계수값이 1에 가깝고
# 다른 종은 0에 가까운 값을 가진 것으로 보임

# 전체 데이터의 평균 실루엣 계수값 계산
avg_sil_score = silhouette_score(iris.data, iris_df['cluster'])
avg_sil_score

# 현재 평균 점수는 0.55라서 1에 가깝도록 군집화 진행

iris_df['silhouette_sample'] = data_per_silhouettes
iris_df.head(1)

# 군집별 실루엣 계수의 평균값
iris_df.groupby(['cluster'])[['silhouette_sample']].mean()

# 1번 군집은 비교적 괜찮음
# 0, 2번 군집은 군집이 약한 것으로 보임

"""- 군집 개수는 2, 3, 4, 5 등 여러 후보군을 넣어서 평가하기
- 새로운 더미 데이터를 생성해서 진행함(make_xxxx 계열)
"""

# 새로운 더미 데이터
from sklearn.datasets import make_blobs

# 파라미터들
# n_samples : 생성할 샘플 수(기본 100)
# n_features : 특성 수(기본 2)
# centers : 군집 중심점의 수(기본 None)
# cluster_std : 군집의 표준 편차(기본 4)
X, y = make_blobs(n_samples = 500, n_features = 2, centers = 4, cluster_std = 1, random_state = 1)
X.shape, y.shape

# 컬러 맵
import matplotlib.cm as cm

# 군집 중심점 개수 후보 목록
candidate_clusters = [i for i in range(2, 6)]

# 군집 평가 시각화 함수 작성
def show_silhouette_per_cluster(candidate_clusters, X) :
  # 군집 후보 개수만큼 시뮬레이션되서 시각화함
  # 한 줄에 차트가 4개 나오도록 하기
  ncols = len(candidate_clusters)
  _, axs = plt.subplots(nrows = 1, ncols = ncols, figsize = (5 * ncols, 5))
  # 군집 후보 개수만큼 반복
  for idx, center_cnt in  enumerate(candidate_clusters) :
    # 1. 알고리즘 생성
    cluster_model = KMeans(n_clusters = center_cnt, max_iter = 500, random_state = 0)
    # 2. 학습과 예측
    cluster_pred = cluster_model.fit_predict(X)
    # 3. 데이터 포인터별 실루엣 계수 계산(그래프의 X축이 실루엣 계수임)
    sil_values = silhouette_samples(X, cluster_pred)
    # 4. 실루엣 계수 평균값 계산(군집이 적절한지 판단하는 기준)
    sil_avg = silhouette_score(X, cluster_pred)
    # 5. 군집화된 데이터 시각화
    # 바닥에는 군집 1번, 그 위는 2번, ...(Y축은 단순 위치값으로만 사용함)
    # 군집을 그릴때 y축의 시작위치임, for문이 다 돌고 군집이 변경되면 0으로 갱신
    y_std_lower = 0
    for i in range(center_cnt) :
      # 1. 군집 번호와 일치하는 실루엣 계수값 추출
      tmp = sil_values[cluster_pred == i]
      tmp.sort() # 정렬
      # 2. 해당 군집에 포함된 실루엣 계수값들을 그리기
      y = np.arange( y_std_lower, y_std_lower + tmp.shape[0]) # 0부터 데이터 포인터 개수만큼|
      x1 = 0 # 0부터
      x2 = tmp # 개별 데이터 포인터의 실루엣 계수값까지 그리기
      # 군집별로 다른 색 설정해줌(0 ~ 1 사이의 값으로 겹치지 않게)
      color = cm.nipy_spectral(i/center_cnt)
      axs[idx].fill_betweenx(y, x1, x2,
                             alpha = 0.6,
                             edgecolor = color,
                             facecolor = color)
      # 3. 해당 군집 번호를 그리기
      axs[idx].text(0.2, y_std_lower + tmp.shape[0] / 2, str(i))
      # 4. 군집 그리는 y축 시작 위치 변경
      # 새로운 위치 = 최초위치 + 현재 군집의 총개수 + 여백(= 10, 설정)
      y_std_lower = y_std_lower + tmp.shape[0] + 10
      
    # 데코레이션
    axs[idx].set_title(f'cluster center count : {center_cnt}')
    axs[idx].set_xlabel('silhouette scoe')
    axs[idx].set_ylabel('cluster id')
    # y축의 좌표계는 제거
    axs[idx].set_yticks([])
    # x축의 좌표계 수정
    axs[idx].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])
    # 6. 실루엣 계수 평균값을 수직으로 시각화
    axs[idx].axvline(x = sil_avg, color = 'red', linestyle = '--')
  plt.show()

# 군집 평가 시각화
show_silhouette_per_cluster(candidate_clusters, X)
# 군집 중심점 개수를 변경해본 결과 중심점이 4개인 것이 가장 적합해보임

candidate_clusters = [i for i in range(2, 6)]
show_silhouette_per_cluster(candidate_clusters, load_iris().data)

# iris 데이터는 중심점이 3개인 것이 가장 적합해보임

"""# Mean-shift (밀도기반)

- 특정 대역폭을 이용해 최초의 확률 밀도 중심이 높은 쪽으로 중심점을 이동하는 방식
- 밀도를 기반으로 군집 중심점을 찾음(가장 높은 밀도를 가진 지점으로 이동할 때까지 반복)
- 데이터 포인터들의 분포도를 이용해서 군집 중심점을 생성함
- 확률 밀도 함수 : KDE(Kernel Desity Estimation)
- 품질 : 대역폭의 크기, 이동 반복 횟수
- 장점
  - 군집을 몇 개로 할 것인지 정할 필요가 없음
  - 대역폭을 통해서 군집이 정해져서 군집화가 유연하게 구성됨
  - 이상치 데이터가 군집에 영향을 미치지 않음
- 단점
  - 대역폭에 따른 성능 차이가 큼
"""

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_meanshfit_1.png')

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_meanshfit_2.png')

# 더미 데이터
X, y = make_blobs(n_samples = 200, n_features = 2, centers = 3, cluster_std = 0.8, random_state = 0)

# 알고리즘
from sklearn.cluster import MeanShift

# 파라미터
# bandwidth : 대역폭
# max_iter : 중심점 최대 이동 횟수
meanshift = MeanShift(bandwidth = 0.9)

# 학습, 예측
cluster_labels = meanshift.fit_predict(X)

# 군집 확인
cluster_labels, np.unique(cluster_labels)

# 군집이 자동으로 8개가 생성됨

# 대역폭을 올려보기
meanshift = MeanShift(bandwidth = 0.9 + 0.1)
cluster_labels = meanshift.fit_predict(X)
cluster_labels, np.unique(cluster_labels)

# 대역폭을 10% 정도 올려보니 군집이 3개로 줄어듬

"""## 최적의 대역폭 찾기

"""

from sklearn.cluster import estimate_bandwidth

# 파라미터
# quantile : 전체 데이터 중 몇 개를 표본으로 사용할지 설정해서 KNN 알고리즘으로 최적의 대역폭 찾음(기본 0.2)
bandwidth = estimate_bandwidth(X, quantile = 0.2)
bandwidth

# 위에서 찾은 최적의 대역폭을 넣고 돌려보기
meanshift = MeanShift(bandwidth = bandwidth)
cluster_labels = meanshift.fit_predict(X)
cluster_labels, np.unique(cluster_labels)

# 군집의 개수가 4개로 나옴

cluster_df = pd.DataFrame(data = X, columns = ['c1', 'c2'])
cluster_df['target'] = y
cluster_df['cluster'] = cluster_labels
cluster_df.head(2)

# 데이터 포인트를 군집기반으로 그리고 각 군집의 중심점에 군집 중심점을 표기
markers = ['o','s','^','x','*']
for c_label in np.unique(cluster_labels) :
  # 1. 특정 군집에 해당되는 데이터를 추출
  tmp = cluster_df[(c_label == cluster_df['cluster'])]
  # 2. 분포도 그리기
  plt.scatter(tmp['c1'], tmp['c2'], edgecolors = 'k', marker = markers[c_label])
  # 3. 중심점 표기
  center_x, center_y = meanshift.cluster_centers_[c_label]
  plt.scatter(center_x, center_y, s = 200, color = 'white', edgecolor = 'k', marker = markers[c_label]) # 배경
  plt.scatter(center_x, center_y, s = 100, color = 'k', edgecolor = 'k', marker = f'${c_label}$') # 글자

plt.show()

# 실제 정답과 비교
cluster_df.groupby('target')['cluster'].value_counts()

"""# GMM(확률기반)

## 설명

- Gaussian mixture model
- 완벽한 좌우대칭
- 정규분포(평균 0, 표준편차 1)를 가진 형태를 가진 가우시안 분포
- 모수 추정 방식 : 개별데이터들이 어떤 정규분포 상에 포함되었는지 결정, 추출하는 방식
"""

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gmm_1.png')

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gmm_2.png')

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gmm_3.png')
# 마치 정규분포 3개가 결합된 형태로 보임
# 데이터는 여러개의 서로 다른 정규분포를 가진 데이터가 합쳐진 형태

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gmm_4.png')
# 아래처럼 생긴 데이터 분포에서 서로 다른 정규분포를 가진 데이터를 추출하는 것

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gmm_5.png')
# 최종 형태 : 개별 정규분포를 가진 군집을 추출해내기

"""## 실습"""

from sklearn.datasets import load_iris
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans

iris = load_iris()
iris_df = pd.DataFrame(data = iris.data, columns = iris.feature_names)
iris_df.head(1)

# GaussianMixture 사용
# n_components : 군집의 개수 => 이 데이터의 정규분포가 몇개인가?
# 후보군을 넣어서 체크
gmm = GaussianMixture( n_components=3, random_state=0)

# 학습
gmm.fit( iris.data )

# 예측
gmm_labels = gmm.predict(iris.data)
# 군집 번호 확인
gmm_labels
# 0, 1번대의 군집은 잘 된 것 같음

# 정답대비, 군집 상황 점검하기 위해 실제 정답, 군집번호 추가
iris_df['tagrget']     = iris.target
iris_df['gmm_cluster'] = gmm_labels
iris_df.head(1)

iris_df.groupby('tagrget')['gmm_cluster'].value_counts()

"""# DBSCAM (밀도기반, 기하학적분포)

## 설명

- Density Based Spatial Clustering of Application with Noise
- 데이터 분포가 일반적이지 않은 기하학적 형태를 띨 경우
"""

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_DBSCAN_1.png')

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gbscan_1.png')
# 데이터가 분포되어 있다

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gbscan_2.png')
# 데이터 포인트를 하나 선정
# 그 포인트를 중심으로 입실론(설정값) 반경으로 원을 그리고
# 그안에 데이터 포인트 4개 이상이면, 군집으로 인정
# 군집으로 인정될때 그 안에 중심점을 코어 포인트라고 한다

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gbscan_3.png')
# p2 확인, 여기를 중심으로 입실론 반경 그림
# 첫번째 반경이 조사되고 나서, 두번째 반경을 조사한 결과
# 포인트가 4개이상 존재 -> 군집으로 인정, 코어 포인트 p2

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gbscan_4.png')
# 2개의 코어 포인트(P1, P2) 가 각각 입실론 반경내에 포함되어 있음
# 그렇다면 2개의 군집은 한개의 군집으로 인정

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gbscan_5.png')
# 한개의 군집으로 인정 -> 군집으 넓어짐

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gbscan_6.png')
# border 포인트
# 새로운 지점에서 반경을 잡음(P3)
# 데이터 포인트가 4개 미만임
# 그런데, 반경내에 코어 포인트가 있음
# 이런 반경은 경계 포인트 표현-> 군집의 경계선

Image('/content/drive/MyDrive/머신러닝/0406_res/cluster_gbscan_7.png')
# 노이즈 포인트
# P5 기준 입실론 반경 조사해본 결과 4개 미만, 코어포인트 없음
# 잡음(이상치)

# 더미 데이터를 기하학적 형태로 제공
from sklearn.datasets import make_circles
# make_circles이용하여 2개원이 겹치지 않게 내부, 외부에 존재하도록 데이터 생성

# factor : 안쪽에 있는원, 바깥쪽 원의 스케일 비율
X, y = make_circles(n_samples=1000, noise=0.05, random_state=0, factor=0.5)
print(X.shape, y.shape)
X

# df 구성
cluster_df = pd.DataFrame( X, columns=['col_1','col_2'])
cluster_df.head(1)

cluster_df['target'] = y
cluster_df.head(1)

"""- DBSCAN, KMeans을 이용한 군집 처리 비교"""

# 알고리즘 준비
from sklearn.cluster import DBSCAN, KMeans

# eps : 입실론 반경값(기본값은 0.5)
# min_samples : 반경 안에 들어가는 데이터 포인터의 수, 이를 기준으로 군집으로 인정함
dbscan = DBSCAN(eps = 0.2, min_samples = 10)
dbscan

# DBSCAN 학습 및 예측 
cluster_df['dbscan_cluster'] = dbscan.fit_predict(X)
cluster_df.head(1)

#  KMeans 학습 및 예측
kmeans = KMeans(n_clusters = 2, max_iter = 1000, random_state = 0)
cluster_df['kmeans_cluster'] = kmeans.fit_predict(X)
cluster_df.head(1)

# 알고리즘별 시각화
def show_per_cluster(df, cols_name, algo) :
  markers = ['o', 's']
  # 중복되지 않는 군집값 추출
  unique_cluster_labels = df[cols_name].unique()
  for label in unique_cluster_labels :
    # 해당 군집값과 일치하는 데이터만 추출
    datas = df[(df[cols_name] == label)]
    # 산포도 그리기
    plt.scatter(x = datas['col_1'], y = datas['col_2'],
                s = 50, edgecolor = 'k', marker = markers[label],
                label = f'cluster - {label}')

  plt.legend()
  plt.show()
  pass

show_per_cluster(cluster_df, 'dbscan_cluster', dbscan)

show_per_cluster(cluster_df, 'kmeans_cluster', kmeans)

