# -*- coding: utf-8 -*-
"""3. 강화학습_정책계산법_정책경사법_살사_큐러닝.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ea2SLur7dHWIuENV_90MTXz-FwFcD7YS

# 개요

- Policy를 갱신하는 기법(정책 계산 방법)
  - 정책 반복법
    - 행동을 중시
    - 행동을 기반으로 정책을 갱신 -> 행동을 유도
  - 가치 반복법
    - 가치를 중시
    - 가치 계산이 핵심 -> 가치를 많이 주는 방향으로 정책이 갱신됨
    - 살사(Sarsa), 큐러닝(Q-Learning), DQN 등

# 미로 게임

## 개요

- 보드 게임
- 게임판 : 3 x 3 정사각형
- 특징
  - 입구와 출구가 각각 1개씩 존재
  - 막힌 사각형의 격자형 칸에 이동할 수 있는 방향이 존재
  - 에이전트(마우스)는 4 방향으로 이동 가능
  - 액션 : 상(0), 우(1), 하(2), 좌(3)
  - 상태 : 미로 게임 격자칸의 위치 정보(0번칸 ~ 8번칸)
  - 정책 : ? (본 노트의 목적)
  - 게임뷰는 탑 뷰
  - 1 step에 1칸씩 이동
- 목적 : 미로 게임판에서 에이전트가 입구에서 출발하여 최단거리로 출구로 나감

## 게임 보드
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline

# 게임의 크기
fig = plt.figure(figsize = (3, 3))

# 외곽 벽 그리기 : 수평선, 수직선 각각 2개씩
plt.plot([0, 3],[3, 3], color = 'k') # 맨위 수평선
plt.plot([0, 3],[0, 0], color = 'k') # 맨아래 수평선
plt.plot([0, 0],[0, 2], color = 'k') # 왼쪽 수직선 -> 입구 표시
plt.plot([3, 3],[1, 3], color = 'k') # 오른족 수직선 -> 출구 표시

# 미로 내부 만들기
plt.plot([1, 1], [1, 2], color = 'k')
plt.plot([2, 3], [2, 2], color = 'k')
plt.plot([1, 2], [1, 1], color = 'k')
plt.plot([2, 2], [0, 1], color = 'k')

# 각 포인트 위치 정보 표시(상태)
n = 0
for i in range(2, -1, -1) :
  for j in range(3) :
    plt.text(j + 0.5, i + 0.5, f'{n}', size = 20, ha ='center', va = 'center')
    n += 1

# 에이전트 표시
# 시작 위치인 0번(0, 0)에 위치함
mouse = plt.plot(0 + 0.5, 2 + 0.5, marker = 'o', markersize = 30, color = 'k')

# 눈금 정리
plt.tick_params(bottom = False, left = False, # - 표시 제거
                labelbottom = False, labelleft = False) # 수치 제거

# 차트 박스 제거
plt.box(False)

# 출력
plt.show()

"""## 요소 설계

|강화 학습 요소|미로 게임|
|--|--|
|에이전트|마우스|
|환경|미로 게임(유한 게임)|
|목적|0번(입구)에서 8번(출구)까지 최단거리 이동|
|행동|4방향(상하좌우)|
|에피소드|0번에서 출발해서 8번에 도착하면 게임 종료|
|상태|에이전트의 위치(0번부터 8번까지)|
|보상|골인 지점까지 이동하는 행동을 중시(정책 경사법으로 처리)|
|정책갱신(학습방법)|정책 경사법|
|파라미터변경주기(정책갱신시퀀스)|- 정책 결정에 영향을 미치는 가중치 담당<br>- 정책은 행동에 영양을 미침<br>- 각 미로상의 지점(상태)에서는 갈 수 있는 곳과 갈 수 없는 곳에 대한 정보를 가지고 있음<br>- 한 에피소드가 종료되면 파라미터를 일괄 갱신(수식참고)|
"""

from IPython.display import Image
Image('/content/drive/MyDrive/딥러닝/rl/미로게임_학습사이클.jpg')

# 정책 계산법에서는 보상없이 행동의 로그를 이용하여 정책을 갱신
# 가치 게산법에서는 각 상태의 가치를 게산해서 정책을 갱신하고 행동
# 아래 이미지는 가치 계산법 상의 미로 게임 사이클

"""## 정책

### 개념

- 에이전트의 상태 정의
  - 미로상에서 에이전트가 취할 수 있는 행동 확률
  - 특정 상태에서 특정 행동을 취할 확률
  - 최초에는 모든 방향에 대해서 동등한 확률을 가짐
  - 이것을 기준으로 정책을 묘사할 수 있음

- 특정 상태(미로상의 특정 포인트)에서 에이전트가 취할수 있는 행동 (갈수있다 = 1, 없다 = nan)

|구분|상|우|하|좌|
|:--:|:--:|:--:|:--:|:--:|
|p0|np.nan|1|1|np.nan|
|p1|:--:|:--:|:--:|:--:|
|p2|:--:|:--:|:--:|:--:|
|p3|:--:|:--:|:--:|:--:|
|p4|:--:|:--:|:--:|:--:|
|p5|:--:|:--:|:--:|:--:|
|p6|:--:|:--:|:--:|:--:|
|p7|:--:|:--:|:--:|:--:|
|p8|:--:|:--:|:--:|:--:|

- 기초 정책을 확률로 표현하면(softmax)

|구분|상|우|하|좌|
|:--:|:--:|:--:|:--:|:--:|
|p0|0|0.5|0.5|0|
|p1|:--:|:--:|:--:|:--:|
|p2|:--:|:--:|:--:|:--:|
|p3|:--:|:--:|:--:|:--:|
|p4|:--:|:--:|:--:|:--:|
|p5|:--:|:--:|:--:|:--:|
|p6|:--:|:--:|:--:|:--:|
|p7|:--:|:--:|:--:|:--:|
|p8|:--:|:--:|:--:|:--:|

### 파라마터 초기값 표현
"""

import numpy as np
# (갈수있다 = 1, 없다 = nan)
theta_zero = np.array( [
  # 상, 우, 하, 좌
  [np.nan, 1, 1, np.nan], # 0번
  [np.nan, 1, 1, 1], # 1번
  [np.nan, np.nan, np.nan, 1], # 2번
  [1, np.nan, 1, np.nan], # 3번
  [1, 1, np.nan, np.nan], # 4번
  [np.nan, np.nan, 1, 1], # 5번
  [1, 1, np.nan, np.nan], # 6번
  [np.nan, np.nan, np.nan, 1], # 7번
])

theta_zero

# np.nansum()을 사용하면 nan을 배제하고 합산
(theta_zero[0]), np.sum(theta_zero[0]), np.nansum(theta_zero[0])
theta_zero[0] / np.nansum(theta_zero[0])

# 직접 구현하는 활성화 함수 : 확률로 계산해서 표현함
def mySoftmax(theta):
  # 결과를 담는 변수
  output = np.zeros_like(theta)

  # 데이터에 큰 수가 존재하고 복잡한 범위를 가진 요소로 구성되어 있다면 지수함수로 일괄처리해서 진행
  theta = np.exp(theta)

  for i in range(theta.shape[0]):
    # 한줄씩 평균을 계산해서 대체
    output[i] = theta[i] / np.nansum(theta[i])
  
  # nan은 0 처리
  return np.nan_to_num(output)

mySoftmax(theta_zero)

"""## 게임 진행 및 정책 계산시 필요한 기능"""

# 정책에 따른 상태를 고려하여, 행동 정보 추출
def getAction(policy, state):
  '''
    policy : 2차원 배열, 정책
    state : 미로 게임상의 위치정보
    특정 위치의 특정 정책은 policy[state]
  '''
  # 특정 위치(상태)에서  특정 정책을 고려하여 행동을 결정
  # 확률을 기반으로 행동을 결정
  # 행동 0, 1, 2, 3
  return np.random.choice([0,1,2,3], p = policy[state])

# 행동에 따른 다음 상태값 정보(포지션) 획득
def getNextState(curState, nextAction):
  '''
    curState : 현재 상태
    nextAction : 다음 행동
  '''
  # 위로 이동
  if nextAction == 0 :
     return curState - 3
  
  # 오른쪽 이동
  elif nextAction == 1 :
     return curState + 1

  # 밑으로 이동
  elif nextAction == 2 :
    return curState + 3

  # 왼쪽 이동
  elif nextAction == 3 :
     return curState - 1

"""## 정책 계산 / 갱신없이 기본 시뮬레이션 진행"""

# 게임이 시작하면 에이전트의 초기 위치
AGENT_FIRST_STATE = 0

# 에이전트 위치가 AGENT_LAST_STATE와 같다면 게임 종료
AGENT_LAST_STATE  = 8

# 최초 정책(확률)
theta = mySoftmax(theta_zero)

# 에피소드 시뮬레이션 함수
def simulation_game(updatedPolicy) :
  # 로그 데이터(2차원 배열)
  # [[상태, 다음 행동], ..., []]
  # 1. 최초 로그
  act_his = [[AGENT_FIRST_STATE, np.nan]]

  # 에이전트 위치
  AGENT_STATE = AGENT_FIRST_STATE
  
  while True :
    # 2. 액션 획득(다음 행동값)
    agent_action = getAction(updatedPolicy, AGENT_STATE)

    # 3. 마지막 로그의 다음 행동값에 따른 위치 정보(상태) 획득 
    next_agent_state = getNextState(AGENT_STATE, agent_action)

    # 4. 현재의 마지막 로그에 다음 행동을 업데이트
    act_his[-1][-1] = agent_action
    
    # 5. 다음 상태에서의 로그 추가(다음 행동은 nan)
    act_his.append(([next_agent_state, np.nan]))

    # 6. 현재 에이전트의 위치를 변경
    AGENT_STATE = next_agent_state

    # 7. 에이전트의 위치가 종료지점(AGENT_LAST_STATE)이라면 종료
    if AGENT_STATE == AGENT_LAST_STATE :
      break

  return act_his

# 에피소드 진행
# act_his : 에이전트가 0에서 8까지 이동한 모든 행동 로그
act_his = simulation_game(theta)

# 로그 확인
act_his

"""## 에이전트 이동 시뮬레이션
- 에이전트가 미로판에서 움직인 로그 재생
"""

from matplotlib import animation
from IPython.display import HTML

def agent_drawing(frame) :
  '''
  frame : 전체 로그 수 만큼 프레임이 0부터 1씩 증가하면서 전달하여 호출됨
  '''
  # 특정 프레임에서의 에이전트의 상태값
  state = act_his[frame][0]

  # 에이전트를 새로운 위치에 그리기
  # 상태값으로 좌표를 획득
  mouse[0].set_data((state % 3) + 0.5, 3 - (state // 3) - 0.5)

ani = animation.FuncAnimation(fig, agent_drawing, frames = len(act_his),
                              interval = 200, repeat = False)
HTML(ani.to_jshtml())

"""## 정책을 갱신해서 최단 거리 이동을 수행해서 미로 탈출

### 정책 경사법 개념(정책 계산법의 하위)
"""

Image('/content/drive/MyDrive/딥러닝/rl/파라미터세타변경량.jpeg')

# s : 특정 상태
# a : 특정 상태에서 취할 특정 행동
# theta(s, a) : 특정 상태에서 특정 행동을 취할 확률(정책)
# eta(학습률) : 설정값 변경량을 몇 %로 적용할지(튜닝의 대상)
# t : 에이전트가 0에서 출발해서 8에 도착할 때까지의 총 행동 수
# N(s, a) : 특정 상태에서 특정 행동을 한 횟수
# P(s, a) : 특정 상태에서 특정 행동을 할 확률
# N(s) : 특정 상테에서 취한 행동의 총 횟수
# 새로운 theta(s, a) = 기존 theta(s, a) + eta x 파라미터 theta의 변경량

"""#### 정책 갱신 함수(정책 계산)"""

def update_thetaPolicy(theta_zero, policy, act_his) :
  '''
  theta_zero : 최초 세타(nan이 존재하는)
  policy : 정책, 확률
  act_his : 에피소드의 행동 로그
  '''
  # 학습률(에타)
  # 일단 세타 변경량의 10%만 정책에 반영
  eta = 0.1

  # 총 행동 횟수 = 총 로그 수 - 1회([8, nan])
  T = len(act_his) - 1
  
  # 상태와 액션의 총 개수 : (8, 4)
  state_count, action_count = theta_zero.shape

  # 파라미터 세타의 변경량 저장할 변수
  paramTheta = theta_zero.copy()
  
  # 세타 변경량 계산
  for s in range(state_count) : # 0 ~ 8
    for a in range(action_count) : # 0 ~ 3
      # 대상이 아닌 것은 제외
      if not theta_zero[s, a] : continue

      # 갱신의 대상인 케이스 : N(s, a), P(s, a), N(s)
      # 특정 상태 s에서 특정 행동 a를 취한 총 개수
      n_sa = len([0 for log in act_his if log == [s, a]])

      # 특정 상태 s에서 특정 행동 a를 취할 확률
      p_sa = policy[s, a]

      # 특정 상태 s에서 행동을 취한 총 개수
      n_s = sum([0 for log in act_his if log[0] == s])

      # 파라미터 세타의 변경량 저장
      # 파라미터 세타를 계산한 값이 각 (s, a) 자리에 저장됨 
      paramTheta[s, a] = (n_sa - p_sa * n_s) / T

  return theta_zero + eta * paramTheta

newTheta = update_thetaPolicy(theta_zero, theta, act_his)
newTheta

"""#### 최종 시뮬레이션

- 에피소드 1000번 적용
"""

SIMULATION_COUNTS = 1000
EPISODES_STOP_VALUE = 10 ** -3
EPISODES_STOP_VALUE

for epi in range(SIMULATION_COUNTS) :
  # 1. 에피소드 진행 : 0에서 출발해서 8에 도착할 때까지 행동하고 로그 리턴
  act_his = simulation_game(theta)

  # 2. 로그로 정책 갱신
  newTheta = update_thetaPolicy(theta_zero, theta, act_his)

  # 3. 정책 변동량 점검
  # 3-1. 새로운 정책을 확률값으로 변환
  newTheta = mySoftmax(newTheta)

  # 3-2. 새로운 정책과 기존 정책 간의 오차값(거리값)의 총합
  policy_dalta = np.sum(np.abs(newTheta - theta))

  # 4. 정책 교체
  theta = newTheta

  # 5. 로그(verbose)
  print(f'에피소드 {epi:4}의 정책변동량 {policy_dalta}')

  # 5. 변동량을 체크해서 미미해지면(EPISODES_STOP_VALUE 미만이면) 종료
  if policy_dalta < EPISODES_STOP_VALUE :
    break

# 최단거리 이동한 행동 로그
act_his

ani = animation.FuncAnimation(fig, agent_drawing, frames = len(act_his),
                              interval = 200, repeat = False)
HTML(ani.to_jshtml())

"""### 가치 계산법 개념

- 가치를 중시하는 정책 갱신 방법
- 행동 1회 단위로 파라미터를 생신
- 행동 수행 -> 상태 변경 -> 가치 변경 ->
정책 갱신
- 알고리즘
  - 살사(Sarsa)
    - 수렴이 느림
    - 국소적인 결론에 갇히지 않음
  - 큐 러닝(Q Learning)
    - 수렴이 빠름
    - 국소적인 결론에 갇힘
  - 위의 알고리즘들은  Q 함수를 갱신하면서 진행함(Q 함수 -> 행렬, 배열, ...)

#### 요소 설계

|강화 학습 요소|미로 게임|
|--:|:--|
|에이전트|마우스|
|환경|미로 게임(유한게임)|
|목적|0번 위치에서(입구) 8번 위치까지(골인/출구) 최단거리 이동|
|행동|4방향 존재, 상우하좌(0123)|
|에피소드|0번에서 출발해서 8번에 도착하면 게임 종료|
|상태|에이전트의 위치(0~8)|
|보상|골인하면 즉시보상 + 1|
|정책갱신(학습방법)|Sarsa, Q Learning|
|파라미터변경주기(정책갱신시퀀스)|1 행동마다 Q 함수 갱신|
|정책|Q 함수|
"""

Image('/content/drive/MyDrive/딥러닝/rl/강화11.png')

"""### 수익, 할인 보상

- 목표 : 수익의 극대화
- 수익 = 즉시보상 + 지연(할인)보상
"""

Image('/content/drive/MyDrive/딥러닝/rl/강화12.png')

Image('/content/drive/MyDrive/딥러닝/rl/강화13.png')

"""### 가치

- 수익
  - 즉시 보상을 제외한 미래에서 얻을 수 있는 수익은 확정되지 않는 값
  - 에이전트는 조건부로 정책을 고정하여 미래의 수입을 게산할 수 있음
  - 이렇게 반영된 수익을 가치라고 부름
  - 이런 가치를 가장 크게 얻을수 있는 조건을 찾아 내는 것이 학습의 목표
    - 구현하기 위한 방법
    - 행동 가치 함수
      - Q 함수
      - Sara, Q Learning, DQN
    - 상태 가치 함수
      - V 함수
      - A2C, Dueling Network

#### 행동 가치 함수의 value 계산 예시
"""

Image('/content/drive/MyDrive/딥러닝/rl/강화14.png')

Image('/content/drive/MyDrive/딥러닝/rl/강화15.png')

# 4번으로 이동하는 바람에 스텝이 2회 추가돼서 가치가 현저하게 줄어들었음
# 0 + γ*0 + γ^2*1 = γ^2*1

Image('/content/drive/MyDrive/딥러닝/rl/강화16.png')

# 4 -> 5 -> 8으로 이동할때 가치 계산

"""#### 상태 가치 함수의 value 계산 예시
- 현재 상태에서 가치를 계산함
- 다음 상태로 행동하는 부분은 배제
"""

Image('/content/drive/MyDrive/딥러닝/rl/강화17.png')

Image('/content/drive/MyDrive/딥러닝/rl/강화18.png')

"""### 벨만 방정식(Bellman Equation)

- 행동 가치 함수, 상태 가치 함수를 수학적으료 표현한 것
- 현재 상태와 다음 상태의 관계를 나타내는 방정식
"""

Image('/content/drive/MyDrive/딥러닝/rl/강화19.png')

Image('/content/drive/MyDrive/딥러닝/rl/강화20.png')

"""### 마르코프 결정과정(Markov Decision Process : MDP)

- 벨만 방정식이 성립하기 위한 환경은 MDP 이여야 함
- 다음 상태가 현재 상태에서 선택한 행동에 따라 확정되는 시스템
- 벨만 방정식으로 행동 가치함수를 학습하는 방법
  - Saras, Q 학습, DQN(Q 학습을 NN(인공신경망)으로 구현한것 DQN이라고 함)
- 벨만 방정식으로 상태 가치함수를 학습하는 방법
  - A2C, Dueling Network
"""



"""### Q 함수 생성

- 행동가치함수로 Q 함수 구현
- Q 함수 = 랜덤값 + theta_zero * 임계값(0.01)
"""

x, y = theta_zero.shape
Q    = np.random.rand(x, y) * theta_zero * 0.01
Q

# 앱실론 적용해서 특정 상태에서 특정 행동을 취하는 함수 새로 생성
def getAction(epsilon, policy, state, Q) :
  # 만약 epsilon 값이 0.1이면 10% 확률로 액션 결정(정책 경사법에서 사용한 선택방법을 활용), 랜덤 행동(확률기반)
  # 아니라면 상태값이 state일 때 Q 함수내에서 최대값을 가진 인덱스 선택, 행동가치함수(Q)
  return np.random.choice([0,1,2,3], p = policy[state]) if  epsilon > np.random.rand() else np.nanargmax(Q[state])

Image('/content/drive/MyDrive/딥러닝/rl/강화21.png')

Image('/content/drive/MyDrive/딥러닝/rl/강화22.png')

# 행동 가치 함수 갱신식 => Q 함수 업데이트 => 정책갱신
# 갱신값 = 현재값 + η(에타:학습률) x TD오차
# TD오차 = 보상을 많이 받는쪽이면(+), 상대적으로 작아지면(-)로 구성되어서 갱신값에 영향을 미침

"""### 행동 가치 함수를 계산하는 알고리즘
- Q 함수 갱신 주기 : 1회 행동 -> 계산 -> Q 함수 갱신

#### Sarsa
"""

def sarsa(Q, s, a, r, s_next, a_next) :
  '''
  Q : 행동가치함수, 갱신의 대상, 정책 기준
  s : 에이전트의 현재 상태(위치)값
  s_next : 에이전트의 다음 상대(위치)값
  a : 현재 상태 행동
  a_next : 다음 상태 행동
  r : 즉시 보상
  '''
  # 학습률 설정 : TD오차의 10%만 행동가치함수 갱신에 영향을 미침
  eta = 0.1

  # 시간할인율(시간지연율) 설정
  gamma = 0.9

  Q[s, a] = Q[s, a] + eta * (r - Q[s, a]) if s_next == AGENT_LAST_STATE else Q[s, a] + eta * (r + gamma*Q[s_next, a_next] - Q[s, a])

  return Q

"""#### Q Learning"""

def q_learning(Q, s, a, r, s_next, a_next) :
  '''
  Q : 행동가치함수, 갱신의 대상, 정책 기준
  s : 에이전트의 현재 상태(위치)값
  s_next : 에이전트의 다음 상대(위치)값
  a : 현재 상태 행동
  a_next : 다음 상태 행동
  r : 즉시 보상
  '''
    # 학습률 설정 : TD오차의 10%만 행동가치함수 갱신에 영향을 미침
  eta = 0.1

  # 시간할인율(시간지연율) 설정
  gamma = 0.9

  # 어떤 행동을 다음번 스텝에서 취해도 관계없음, 오직 값이 높은 행동만 선택
  # 이것 때문에 수렴도 빠르지만 구소적일수 있음
  Q[s, a] = Q[s, a] + eta * (r - Q[s, a]) if s_next == AGENT_LAST_STATE else Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next, : ]) - Q[s, a])
  
  return Q

"""### 행동가치함수 통합 : 클로저(함수 안에 함수)
- 클로저 조건
  - 외부 함수가 있고 내부 함수가 있음
  - 내부 함수가 최종 리턴되는 형태
  - 함수 자체는 pure해야 함(어떤 코드에 들어가도 작동 가능한 형태, 전역변수가 함수 내부에 개입되지 않음)
- 클로저 확장 개념 == 데코레이터(@~~)
"""

# 클로저 예시
def q_update_func(Q, s, a, r, s_next, a_next, type = 'sarsa') :
  def sarsa(Q, s, a, r, s_next, a_next) :
    

    return Q

  def q_learning(Q, s, a, r, s_next, a_next) :
    

    return Q

  return sarsa(Q, s, a, r, s_next, a_next) if type == 'sarsa' else q_learning(Q, s, a, r, s_next, a_next)

"""### 시뮬레이션"""

def game_play(epsilon, Q, policy) :
  # 초기 상태
  s = AGENT_FIRST_STATE

  # 이동 로그
  act_his = [[s, np.nan]]

  # 초기 행동
  a_next = getAction(epsilon, policy, s, Q)

  # 에이전트가 8번 위치에 도착할 때까지 반복
  while True :
    # 1. 다음 행동을 현재 행동으로 변경
    a = a_next

    # 2. 현재 상태에서 취할 행동으로 인한 다음 상태값 획득
    s_next = getNextState(s, a)

    # 3. 로그 한 개 작성
    act_his[-1][-1] = a

    # 4. 새로운 로그 추가 : 상태는 그대로 넣고 행동은 아직 안 해서 nan 값으로 넣음
    act_his.append([s_next, np.nan])

    # 5. 가치 계산을 위한 보상 체그
    # 다음 상태(위치)가 골인 지점이면 즉시보상은 1, 아니면 즉시보상은 0임
    r = 1 if AGENT_LAST_STATE == s_next else 0
    # 다음 상태(위치)가 골인 지점이면 다음 행동은 없음
    a_next = np.nan if AGENT_LAST_STATE == s_next else getAction(epsilon, policy, s_next, Q)

    # 6. 행동가치함수 갱신(Q 갱신)
    # Q = sarsa(Q, s, a, r, s_next, a_next)
    Q = q_learning(Q, s, a, r, s_next, a_next)

    # 에이전트의 다음 상태값이 최종 위치(8번)이면 종료
    if AGENT_LAST_STATE == s_next :
      break
    else :
      s = s_next

  return act_his, Q

policy = mySoftmax(theta_zero)

# 시뮬레이션
# 엡실론을 50%로 줘서 50% 확률로 랜덤하게 선택
epsilon = 0.5

# 에피소드는 10(게임 10번만 함)
for epi in range(10) :
  # 1. 에피소드 진행
  act_his, Q = game_play(epsilon, Q, policy)

  # 2. 로그
  print(f'에피소드 {epi + 1 : 2} / 에이전트 이동수 {len(act_his) : 3} / 엡실론 {epsilon}')

  # 3. 에피소드가 종료될 때마다 엡실론 값을 50% 줄이기
  epsilon /= 2

# 각 위치(상태)에서 골인에 최단거리로 갈 수 있는 이동 방향(상하좌우)의 가치 보기
Q

"""### 결론

- 게임이 복잡해지면 Q 함수도 같이 커지고 이를 처리하는 연산량도 같이 증가하는 문제가 발생함
- 복잡한 문제는 딥러닝을 활용해서 처리함
"""