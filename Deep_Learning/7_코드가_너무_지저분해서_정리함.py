# -*- coding: utf-8 -*-
"""7.  코드가 너무 지저분해서 정리함.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HW5J4AlSPrZcQ3Vh822KgdOE27GGvxMd
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install tensorflow==1.15

# %tensorflow_version 1.x

import tensorflow as tf
tf.__version__

import numpy as np
from tensorflow.examples.tutorials.mnist import input_data

# 데이터 획득
mnist = input_data.read_data_sets('./data/mnist', one_hot = True)

# 환경변수 설정
PIXEL = mnist.train.images.shape[1]
PIXEL_H = int(np.sqrt(PIXEL))
PIXEL_W = PIXEL_H
LABEL_NUM = mnist.train.labels.shape[-1]

x = tf.placeholder(tf.float32, shape = (None, PIXEL), name = 'x')

def createFilterByWeight(name, shape) :
  name = f'{name}_W'
  initial_value = tf.truncated_normal(shape, stddev = 0.1)
  W = tf.Variable(initial_value = initial_value, name = name)
  return W

def createBias(name, shape, value) :
  name = f'{name}_b'
  initial_value = tf.constant(value, shape = shape)
  b = tf.Variable(initial_value = initial_value, name = name)
  return b

def createConv2D(name, x, W) :
  name = f'{name}_conv'
  return tf.nn.conv2d(x, filter = W, strides = [1, 1, 1, 1], padding = 'SAME', name = name)

# 합성곱 1층 생성
conv_1f_W = createFilterByWeight('1f_conv', (5, 5, 1, 32))
conv_1f_b = createBias('1f_conv', (32, ), value = 0.1)
x_4d = tf.reshape(x, (-1, PIXEL_H, PIXEL_W, 1))
conv_1f = createConv2D('1f', x_4d, conv_1f_W) + conv_1f_b
act_conv_1f = tf.nn.relu(conv_1f)

def createMaxPooling(name, x) :
  return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME', name = f'{name}_max')
  
# 풀링 1층 생성
pool_1f = createMaxPooling('pooling_1f', act_conv_1f)

# 합성곱 2층 생성
conv_2f_W = createFilterByWeight('2f_conv', (5, 5, 32, 32 * 2))
conv_2f_b = createBias('2f_conv', (32 * 2, ), value = 0.1)
conv_2f = createConv2D('2f', pool_1f, conv_2f_W) + conv_2f_b
act_conv_2f = tf.nn.relu(conv_2f)

# 풀링 2층 생성
pool_2f = createMaxPooling('pooling_2f', act_conv_2f)

# 전결합층 생성
_, h, w, ch = pool_2f.shape
in_channels = h * w * ch
out_channels = 1024
fc_x = tf.reshape(pool_2f, (-1, in_channels))
fc_W = createFilterByWeight('fc', (in_channels, out_channels))
fc_b = createBias('fd', (out_channels, ), 0.1)
fc = tf.matmul(fc_x, fc_W) + fc_b
act_fc = tf.nn.relu(fc)

# 드롭아웃층
keep_prob = tf.placeholder(tf.float32)
act_fc_dropout = tf.nn.dropout(act_fc, rate = 1 - keep_prob)

# 출력층 생성
_, in_ch = act_fc_dropout.shape
y_W = createFilterByWeight('output', (in_ch, LABEL_NUM))
y_b = createBias('output', (LABEL_NUM, ), 0.1)
y_conv = tf.matmul(act_fc_dropout, y_W) + y_b
y_conv = tf.nn.softmax(y_conv)

# 실제 정답
y_ = tf.placeholder(tf.float32, shape = (None, LABEL_NUM), name = 'y_')

# 손실함수
cross_entropy = - tf.reduce_sum(y_ * tf.log(y_conv))

# 최적화
optimizer = tf.train.AdamOptimizer()

# 훈련도구
train = optimizer.minimize(cross_entropy)

# 예측
predict = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))

# 정확도
accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))

# 데이터 주입용 함수
def createFeedDict(x_train_data, y_train_label, prob) :
  return {
      x : x_train_data,
      y_ : y_train_label,
      keep_prob : prob
  }

# 학습
losses = list()
with tf.Session() as sess :
  TRAIN_TOTAL_COUNT = 2500
  BATCH_SIZE = 64
  VERBOSE_INTERVAL = 100
  sess.run(tf.global_variables_initializer())
  for step in range(TRAIN_TOTAL_COUNT) :
    x_batch = mnist.train.next_batch(BATCH_SIZE)
    train_fd = createFeedDict(x_batch[0], x_batch[1], 0.1)
    acc, _, loss = sess.run([accuracy, train, cross_entropy], feed_dict = train_fd)
    losses.append(loss)
    if step % VERBOSE_INTERVAL == 0 :
      test_fd = createFeedDict(mnist.test.images, mnist.test.labels, 1.0) 
      acc = sess.run(accuracy, feed_dict = test_fd)
      print(f'step : {step : 4} / acc : {acc : 18} / lost : {loss : 18}')
  test_fd = createFeedDict(mnist.test.images, mnist.text.labels, 1.0)
  acc = sess.run(accuracy, feed_dict = test_fd)
  print(f'step : {step : 4} / acc : {acc : 18} / lost : {loss : 18}')

