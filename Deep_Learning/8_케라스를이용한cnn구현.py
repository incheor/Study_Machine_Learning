# -*- coding: utf-8 -*-
"""8. 케라스를이용한CNN구현.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eyGp4dhKXiO0PnriT7hHxC4W-JkNIJ25

# 개요

- 딥러닝을 몇 개의 API 사용만으로 손쉽게 구현할 수 있는 고수준  API 인터페이스
- keras는 백엔드 엔진을 지정하여서 딥러닝을 수행
  - tensorflow에서 2017 commit 행사에서 공식적으로 keras를 하위 패키지로 포함
  - tensorflow 2.x에서 개발방법론중 고수준 개발 방식으로 지정하여 권장하고 있음
"""

# 케라스 임포트하고 버전 확인
from tensorflow import keras
keras.__version__

"""# 난수 고정

- 일반적으로 어떤 엔진을 사용하던 동일한 결과를 얻기 위한(재현성) 조치사항
- 단 GPU 학습시 시드 고정이 풀리는 문제가 일부 있음
"""

# 시드값 준비
RAND_SEED_VALUE = 0

# os레벨
import os
os.environ['PYTHONJASHSEED'] = str(RAND_SEED_VALUE)

# 파이썬 레벨
import random
random.seed(RAND_SEED_VALUE)

# numpy 레벨
import numpy as np
np.random.seed(RAND_SEED_VALUE)

# 엔진 레벨
import tensorflow as tf
# 1.x 시드 고정
# tf.random.seed(RAND_SEED_VALUE)
# 2.x 시드 고정
# tensorflow._api.v2.random.seed

# 케라스 레벨(생략)

"""# 데이터 형식 확인"""

from tensorflow.keras import backend

backend.image_data_format()
# channels_last : NHWC (텐서플로우)
# channels_first : NCHW (파이토치)

"""# 데이터 준비

- 케라스에서 데이터 받기
- 데이터는 NHWC 형식(4D)으로 구성
- MNIST => ( -1, 28, 28, 1 )
"""

from tensorflow.keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train.shape, y_train.shape, X_test.shape, y_test.shape

# 환경변수 획득
PIXEL_H = X_train.shape[1] # 28
PIXEL_W = X_train.shape[2] # 28
PIXEL = PIXEL_H * PIXEL_W  # 784
LABEL_NUM = len(np.unique(y_train)) # 10
IN_CHANNEL= 1 # 입력채널수(grayscale)

# 데이터 포멧에 맞춘 shape 변경
if 'channels_last' == backend.image_data_format() : # NHWC
  # (6000, 28, 28, 1)
  X_train = X_train.reshape(-1, PIXEL_H, PIXEL_W, IN_CHANNEL)
  X_test = X_test.reshape(-1, PIXEL_H, PIXEL_W, IN_CHANNEL)

else:
  # (6000, 1, 28, 28)
  X_train = X_train.reshape(-1, IN_CHANNEL, PIXEL_H, PIXEL_W)
  X_test = X_test.reshape(-1, IN_CHANNEL, PIXEL_H, PIXEL_W)

# 형변환 (0.0 ~ 255.0)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# 정규화 처리 (0.0 ~ 1.0)
X_train = X_train / np.max(X_train)
X_test = X_test / np.max(X_test)

# 정답 (one-hot 인코딩 처리)
y_train = keras.utils.to_categorical(y_train, LABEL_NUM)
y_test = keras.utils.to_categorical(y_test,  LABEL_NUM)
y_train.shape, y_test.shape

"""# 신경망 구성

- model.add(레이어)
"""

# 1. 네트워크 구성 준비, model에  layer을 추가하면서 신경망 구성이 완료된다
from tensorflow.keras import models, layers

model = models.Sequential()

if 'channels_last' == backend.image_data_format():
  # HWC   
  input_shape = (PIXEL_H, PIXEL_W, IN_CHANNEL)  
else: 
  # CHW
  input_shape = (IN_CHANNEL, PIXEL_H, PIXEL_W)

# 2. 합성곱 1층 
model.add(layers.Conv2D(
    filters = 32, # 합성곱층을 통과한 결과물의 출력 채널의 수 1 -> 32 
    kernel_size = (5, 5), # 가중치(w)를 공용파라미터로 가진 커널의 크기(정방형 2D 행렬)
    strides = (1, 1), # 세로, 가로의 이동량 각각 1
    padding = 'same', # 소문자로 표현(저수준은 대문자), same 동일크기, vaild 유효범위(축소)
    activation = 'relu', # 활성화 함수 relu(입력이 음수면 0, 입력이 양수면 그대로)
    input_shape = input_shape # 최초 입력 x가 들어오는 자리이므로 입력 형태를 표현(최초층이므로 추가)
))

# 3. 풀링 1층
model.add(layers.MaxPool2D(
    pool_size = (2, 2), # 커널 크기를 지정(정방형)
    strides = None, # None을 주면, 커널크기와 동일
    padding = 'same' # 동일 크기-> k, s, p, x 등은 공식이 성립
))
model.add(layers.Dropout(0.1))

# 4. 합성곱 2층 
model.add(layers.Conv2D(32 * 2, (5, 5), padding = 'same', activation = 'relu'))

# 5. 풀링 2층
model.add(layers.MaxPool2D(padding = 'same'))
model.add(layers.Dropout(0.1))

# 6. 전결합층 : 특성이 없는층은 Dense()로 표현
model.add(layers.Flatten())
model.add(layers.Dense(1024, activation = 'relu'))

# 7. 과적합방지층(드롭아웃층)
model.add(layers.Dropout(0.1))

# 8. 출력층
model.add(layers.Dense(LABEL_NUM, activation = 'softmax'))

"""# 학습, 최적화, 예측 구성

- model.compile(도구)
"""

model.compile(
  # 최적화 도구
  optimizer = 'rmsprop',
  # 손실함수
  loss = keras.losses.categorical_crossentropy,
  # 정확도
  metrics = ['accuracy']
)

"""# 학습

- model.fit(파라미터)
"""

# 학습 관련 변수 설정
EPOCHS = 10  # 10세대 학습을 진행, 전체 데이터풀로 학습에 동원(10회)
BATCH_SIZE = 128 # 설정값 (앞선 과정에서는 50을 넣었음)

if 0 : 
  history = model.fit(
    x = X_train, # 훈련 데이터
    y = y_train, # 정답 데이터
    batch_size = BATCH_SIZE, # 한번의 학습시 동원되는 데이터량
    epochs = EPOCHS, # 세대학습수 세팅 
    # verbose = 'auto',# 로그 출력
    # callbacks = None, # 조기학습 종료등 콜백함수등록
  )

"""# 학습개선-조기학습종료 도입

- 학습 성과가 더이상 나오지 않을때 추가되는 학습은 무의미함
- 변동폭이 더이상 진행되지 않으면 학습을 종료시킴(조기학습 종료)
- 콜백함수 등록후 처리
"""

from tensorflow.keras.callbacks import EarlyStopping

# 조기 학습 종료 객체
early_stopping = EarlyStopping()

with tf.device('/device:GPU:0') : 
  history = model.fit(
    x = X_train, # 훈련 데이터
    y = y_train, # 정답 데이터
    batch_size = BATCH_SIZE, # 한번의 학습시 동원되는 데이터량
    epochs = EPOCHS, # 세대 학습수 세팅 
    # verbose = 'auto', # 로그 출력
    callbacks = [early_stopping], # 조기학습 종료등 콜백함수 등록
    validation_split = 0.2 # 검증 데이터 비율    
  )

# 이 정보를 이용하여 시각화 가능하다
history.history

"""# 모델 덤프"""

model.save('mnist_cnn.h5')

"""
# 모델 로드"""

from tensorflow.keras.models import load_model

loadedModel = load_model('mnist_cnn.h5')

# 테스트 모델을 넣어서 예측 수행
loadedModel.evaluate(X_test, y_test)