# -*- coding: utf-8 -*-
"""2.강화학습_다중슬롯머신구현_기본강화학습요소_및_매커니즘이해.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mSbcnQQXaOSpsnjF0jpKZREBHgupK5EI

# 강화학습 실습할 내용

- 다중 슬롯 머신
  - 가치 계산용 알고리즘
    - UCB1
    - 엡실론 그리디
  - 강화 학습 입문형

- 미로게임
  - 정책 계산(정책 반복법, 가치 계산법)
    - 알고리즘
      - 정책 경사법
      - 살사
      - 큐 러닝

- 카드 풀 게임
  - openai gym에서 제공함
  - 정책 계산(정책 반복법, 가치 계산법)
    - DQN 활용(tensorflow + keras)

# 다중 슬롯 머신 실습

## 목적

- 목적
  - 강화학습 스타일일 익힘
  - 상화학습의 절차 습득 및 요소, 학습 스타일 습득

- 환경
  - 게임의 룰
  - 슬롯 머신의 팔이 여러개 존재할 수 있음(현실에서 슬롯 머신의 팔은 항상 1개임)
  - 각 팔의 보상(reward)은 다름, 각 팔의 확률은 정해져 있음
    - 1번 팔 : 10% 확률로 보상 제공
    - 2번 팔 : 50% 확률로 보상 제공
  - 보상 확률은 게임이 세팅되는 정해지고 에이전트는 각 팔에 대한 보상 확률을 모름
  - 보상의 크기는 중요하지 않음, 보상을 받는지 못 받는지가 중요함

- 에이전트의 목적
  - 어떤 팔을 선택해야 최대 보상을 받는지 그 팔을 찾아내기 위해 학습을 함

- 에이전트의 행동
  - 여러개의 팔 중 한 개를 선택함
  - 확률에 의해서 보상을 지급(1 or 0)
  - 1 에피소드 종료(1 스텝으로 1 에피소드 종료)
  - 상태 변화 없음

|강화 학습 요소|다중 슬롯 머신 게임|
|--|--|
|에이전트|슬롯 머신의 팔을 내리는 사람|
|환경|다중 슬롯 머신|
|목적|많은 보상 획득|
|행동|여러 개의 팔 중 한 개를 선택|
|에피소드|팔을 당기면 게임 종료|
|상태|없음|
|보상|1 or 0|
|수익|보상의 총합(즉시보상 + 지연보상(0, 선택하면 종료이기 때문에 미래 보상이 없음))
|학습방법|UCB1, 엡실론 그리디|
|파라미터 갱신 주기|1회 행동으로 개임이 종료되면 즉시 진행|
|정책|다음의 슬롯 머신 팔 선택시 판단용|

## 정책

- 탐색과 이용의 트레이드 오프 문제
  - 정보 수집을 위해서 탐색을 수행하면 언젠가는 모든 팔에 대한 정보를 알게 됨
  - 만약 횟수가 제한되어 있다면
    - 확률이 가장 높은 팔을 찾다가 횟수를 소진하면 수익이 적을 수 있음
    - 적당한 확률을 찾아서 그 팔만 선택한다면 찾지 못한 확률이 더 높은 팔을 선택하는 것 보다 수익이 적을 수 있음
    - 따라서 적절한 균형을 찾아야 함

## 구현

### 구조

```
- SlotMachineGame
  L SlotArm Class : 슬롯 머신의 팔
  L GameEngine Class : 알고리즘 사용의 표준 인터페이스 제공
    L EpsilonGreedyEngine Class : 알고리즘 1
    L UCB1Engine Class : 알고리즘 2
  L GameSimulator Class or def : 게임 가동, 시뮬레이션 진행
```

### 필요 모듈
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import random
import math
import pandas as pd
import matplotlib.pyplot as plt

# %matplotlib inline

"""### SlotArm Class"""

# 다중 슬롯 머신의 팔을 구현
# 모든 팔은 같은 형태를 하고 있기 때문에 클래스로 구현
# 각 팔은 서로다른 보상 확률을 가지고 있음
class SlotArm :
  '''
    __init__(self, p), p는 확률(0.0 <= x <= 1.0)
    reward(self)는 p 확률 이하로 보상을 지급(1.0), 미지급(0.0)    
  '''
  # 생성자 : 객체를 생성할 때 호출, 이름은 고정임(__init__)
  # 아래에서는 생성자를 재정의함(오버라이드)
  def __init__ (self, p) :
    self.p = p
  
  # 멤버 변수
  p = None

  # 멤버 함수
  def reward(self) :
    # 삼항 연산자
    # return 1.0 if self.p > random.random() else 0.0
    if self.p > random.random() :
      return 1.0
    else :
      return 0.0

# 50% 확률로 보상을 주는 팔 만들기
SlotArm(0.5)

"""### GameEngine Class

- 객체 지향 프로그램의 상속성을 활용하여 2개의 알고리즘을 동일한 인터페이스 형태로 구현
- 테스트 코드는 for문에서 동일한 함수를 호출하는 것으로 비교할 수 있음
"""

class GameEngine():
  # 알고리즘 초기화
  def initalize(self) :
    pass

  # 팔을 당김
  def pull_arm(self) :
    pass

  # 보상 업데이트
  def update_value(self) :
    pass

  # 알고리즘 정보
  def get_engine_info(self) :
    pass

"""#### Epsilon Greedy Engine

- Epsilon Greedy 알고리즘
  - 확률 e를 기준으로 랜덤하게 행동을 선택함
    - e(0 ~ 1) 또는 1 - e를 기준으로 랜덤하게 행동을 선택
    - 에이전트가 팔을 선택하는 행위
  - 통상적으로 0, 1을 사용할 경우 가장 높은 성능을 보임
  - 탐색과 이용에 대한 트레이드 오프 처리를 해야 함
    - 균형
    - 탐색 : e
    - 이용 : 1 - e
"""

from IPython.display import Image
Image('/content/drive/MyDrive/딥러닝/rl/ε-greedy.jpeg')

# 가치를 계산하는 식
# 현재 시점의 행동의 가치 = (처음부터 직전까지 시도한 회수 / 전체 시행 회수) * 이전 행동까지의 가치 +
#                           (1 / 전체 시행 회수) * 이번 행동으로 얻는 보상

# EpsilonGreedyEngine 클래스 생성, 부모클래스(GameEngine) 상속
class EpsilonGreedyEngine(GameEngine) :
  '''
  EpsilonGreedy를 활용한 팔 선택 엔진
  epsilon 기본값은 0.1
  '''
  # 생성자
  def __init__(self, _epsilon = 0.1) :
    self.epsilon = _epsilon

  # 알고리즘 초기화
  def initalize(self, count_arms) :
    # 모든 요소가 0인 배렬(행렬) 생성 : np.zeros()
    # 특정 팔이 몇 번 선택되었는지 기록하는 배열
    # self.n의 총합은 팔을 선택한 총 횟수, 전체 에피소드 수, 전체 플레이 횟수
    self.n = np.zeros(count_arms)

    # 각 팔에 대한 보상값 기록하는 배열
    self.v = np.zeros(count_arms)

  # 팔을 당김
  def pull_arm(self) :
    # 10% 확률로 탐색 / 90% 확률로 이용 -> 팔들 중에서 보상값이 가장 높은 팔
    # np.argmax : 배열 중 가장 큰 값을 가진 요소의 인덱스를 리턴함
    return np.random.randint(0, len(self.n)) if self.epsilon > random.random() else np.argmax(self.v)

  # 보상 업데이트
  # 특정 팔을 당겼을 때의 보상, 선택 횟수 업데이트
  def update_value(self, choice_arm, reward) :
    # 1. 특정 팔 선택 회수 증가
    self.n[choice_arm] += 1
    # 2. 특정 팔 가치 계산
    n = self.n[choice_arm]
    v = self.v[choice_arm]
    self.v[choice_arm] = ((n - 1) / n) * v + (1 / n) * reward

  # 알고리즘 정보
  def get_engine_info(self) :
    return 'Epsilon Greedy를 이용한 팔 선택 엔진'

EpsilonGreedyEngine()

"""#### UCB1 Engine

- UCB1 원리
  - 모든 팔을 1회 이상 사용할 때까지 가치 계산, 갱신하지 않고 전체 팔을 탐색함
  - 만약 모든 팔이 1회 이상 선택되었다면 팔의 가치 계산을 함
"""

Image('/content/drive/MyDrive/딥러닝/rl/UCB1.jpeg')

# 아래 그림은 오류임, 제곱근(루트)이 있는데도 1/2 제곱(이것도 루트)이 있음, 둘 중 하나만 보기
# UCB1 = 성공률 + 바이어스(편향값, 미세소정)
# t값 때문에 팔을 하나 선택하면 모든 팔의 가치를 재조정해야 함

class UCB1Engine(GameEngine):
  # 알고리즘 초기화
  def initalize(self, count_arms) :
    # 각 팔의 시행 횟수 저장 배열
    self.n = np.zeros(count_arms)

    # 각 팔의 보상 저장 배열
    self.v = np.zeros(count_arms)

    # 각 팔의 보상을 받은 횟수를 저장하는 배열
    self.w = np.zeros(count_arms)

  # 팔을 당김
  def pull_arm(self) :
    # 모든 팔을 당길 때까지 계속 탐색해서 미선택 팔이 있으면 리턴
    for i in range(len(self.n)) :
      # 한번도 선택되지 않은 팔의 인덱스 리턴
      if not self.n[i] :
        return i
    # 만약 모두 선택됨 -> 이용 -> 가치가 가장 높은 팔 선택
    # 우연히 동점이 나오면 그 중 가장 앞에 있는 인덱스가 나옴
    return np.argmax(self.v)

  # 보상 업데이트 : 모든 팔이 선택되었다면 가치 계산 시작
  def update_value(self, choice_arm, reward) :
    # 팔을 선택했을 때 증가
    self.n[choice_arm] += 1

    # 보상을 받은 횟수 증가
    if reward :
      self.w[choice_arm] += 1

    # 모든 팔이 최소 1회 이상 선택되었는지 확인
    for i in range(len(self.n)) :
      # 만약 아직 선택되지 않은 팔이 있으면 가치 계산 안 함
      if not self.n[i] :
        return 
    
    # 가치 계산, 가치 갱신
    # 모든 팔을 계산해야 함
    for i in range(len(self.v)) :
      # 성공률
      suc_rate = self.w[i] / self.n[i]
      
      # 모든 팔의 시행횟수
      t = self.n.sum()
      
      # 바이어스
      vais = (2 * math.log(t)/ self.n[i]) ** 0.5
      self.v[choice_arm] = suc_rate + vais

  # 알고리즘 정보
  def get_engine_info(self) :
    return 'UCB1 알고리즘을 이용한 팔 선택 엔진'

UCB1Engine()

"""## 시뮬레이션"""

# 시각화시 한글 깨짐 방지
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

from os import times
def play(engine, slot_arms, sim_cnt, epi_cnt) :
  # 게임은 1000 * 250회 진행
  # 총 시도 횟수
  times = np.zeros(sim_cnt * epi_cnt)

  # 보상
  rewards = np.zeros(sim_cnt * epi_cnt)

  # 시뮬레이션 진행
  for sidx in range(sim_cnt) :
    # 게임 엔진 초기화 -> 250회 팔 선택 -> 종료 => 총 1000번 수행
    # 팔의 개수를 넣어서 엔진을 초기화함
    engine.initalize(len(slot_arms))
    for step in range(epi_cnt) :
      # 0. 시도 횟수 기록
      offset = epi_cnt * sidx
      index = offset + step
      times[index] = step + 1
      
      # 1. 팔을 당김
      cur_arm_index = engine.pull_arm()
      
      # 2. 보상 계산
      reward = slot_arms[cur_arm_index].reward()

      # 3. 계산된 가치 저장
      rewards[index] = reward

      # 4. 가치 업데이트
      engine.update_value(cur_arm_index, reward)

  return times, rewards

# 시각화시 한글 깨짐 방지
plt.rc('font', family='NanumBarunGothic') 

# 전체 시뮬레이션 횟수
SIMULATION_COUNT = 1000

# 에피소드 횟수(시뮬레이션 1회당 250회)
EPISODE_COUNT = 250

# 게임 구성
slot_arms = [
  SlotArm(0.3),             
  SlotArm(0.5),
  SlotArm(0.9),
]
engines = [
  EpsilonGreedyEngine(),
  UCB1Engine()
]

# 시뮬레이션 진행
for engine in engines :
  # 1. 시뮬레이션
  times, rewards = play(engine, slot_arms, SIMULATION_COUNT, EPISODE_COUNT)

  # 2. 결과를 데이터 프레임화
  # x축 : 시도한 횟수, y축 : 보상
  df = pd.DataFrame({
      'times' : times,
      'rewards' : rewards
  })

  # 3. times(1 ~ 250)를 그룹화하고 평균으로 조정해서 시각화
  tmp = df.groupby('times').mean()

  # 4. 시각화
  plt.plot(tmp, label = engine.get_engine_info())

  # 5. 디자인
  plt.xlabel('Episode')
  plt.ylabel('Reward')
  plt.legend()

# 출력
plt.show()

