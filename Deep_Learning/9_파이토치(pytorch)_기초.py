# -*- coding: utf-8 -*-
"""9. 파이토치(Pytorch) 기초.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11cOH71trDx2ce5capD2YwB32RSdWpqcu

# 개요

- Define By Run : 매 단계를 바로 확인 가능함 -> 디버깅에 유리
- 연구기관 등에서 사용됨
- 객체지향적 코드를 많이 사용함

# 모듈 가져오기
"""

import torch
torch.__version__

from IPython.display import Image
Image('/content/drive/MyDrive/딥러닝/dl/tensor_style.jpeg', width = 800)

"""# 텐서 생성

- 리스트, 배열로 만들 수 있음
"""

# 리스트로 해보기
# 일단 리스트 생성
list_src = [
  [1, 2, 3],
  [11, 12 ,13]
]
print(f'리스트 : {list_src}')

# 구동 즉시 텐서가 생성되고 데이터가 세팅됨 -> 바로 확인 가능 -> 디버깅에 유리
x = torch.Tensor(list_src)
print(f'텐서 : {x}')

# 텐서에서 데이터를 원상복구해보기
# 텐서 -> 리스트
print(f'텐서 -> 리스트 : {x.tolist()}')

# 배열로 해보기
# 배열 생성
import numpy as np

arr = np.array(list_src)
print(f'배열 : {arr}')

# 배열로 부터 텐서 생성, 기본타입으로 float 처리됨
x = torch.Tensor(arr)
print(f'텐서 : {x}')

# 텐서에서 배열로 
print(f'텐서 -> 배열 : {x.numpy()}')

# 난수를 원소로 가진 텐서 생성
# 아래는 shape : (2, 3) 원소는 0 이상 5 미만
torch.randint(0, 5, size = (2, 3))

# 영행렬인 텐서 생성
torch.zeros(2, 2)

# 구성원이 모두 1인 행렬인 텐서 생성
torch.ones(2, 2)

# 텐서 x에 shape을 본따서 영행렬인 텐서 생성
torch.zeros_like(x)

"""# GPU 사용

- 텐서별로 GPU를 지정할 수 있음
- 단위별로 적용이 가능
- 연계된 텐서들은 한가지 방식으로 학습해야 연동됨
"""

# 엔비디아 GPU가 지원되는지 확인
torch.cuda.is_available()

# 안 됨

# CPU로 작업하기 위한 설정
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'DEVICE : {DEVICE}')

# 특정 텐서에 적용
print(f'텐서에 적용 : {x.to(DEVICE)}')

# 장비 개수 체크
torch.cuda.device_count()

"""# 텐서 타입"""

from IPython.display import Image
Image('/content/drive/MyDrive/딥러닝/dl/torch2.png', height = 600)

# 리스트로 배열만들고 그 배열로 텐서를 만듬
a = torch.FloatTensor(np.array([1, 2, 3, 4]))
print(f'텐서 : {a}')

# 텐서 기본 속성 확인
type(a), a.type(), a.dtype, a.shape, a.size()

# 이번에는 Bool 형으로 해보기
a = torch.BoolTensor(np.array([True, False]))
type(a), a.type(), a.dtype, a.shape, a.size()

"""# 텐서의 조작 및 연산 함수

- 텐서를 만나면 바로 체크해야할 내용
  - 모양 : size(), shape
  - 차원 : dim(), ndim
  - 타입 : type(), dtype
"""

# torch의 난수 고정
torch.manual_seed(1024)

# 텐서가 생성
x = torch.randint(0, 10, size = (2, 3, 4))
print(f'텐서 : {x}')

# 텐서가 생성되면 바로 체크할 포인트
print(f'x.type() : {x.type()}, x.dim() : {x.dim()}, x.size() : {x.size()}')

# Commented out IPython magic to ensure Python compatibility.
# 텐서 시각화 함수
def draw3DTensor(src) :
  import matplotlib.pyplot as plt
#   %matplotlib inline
  
  fig, axes = plt.subplots(len(src), 1)

  for i in range(len(src)) :
    # 2차원 텐서 시각화 : matshow()
    # 근데 지금 텐서 x는 3차원이라서 슬라이싱해줘야 함
    axes[i].matshow(src[i].numpy(), vmin = 0, vmax = 1, cmap = 'gray')
  plt.show()

"""## indexing, slicing

- 텐서에서 특정 데이터를 추출
  - 인덱싱 : 차원 축소
  - 슬라이싱 : 차원 유지
"""

# 인덱싱
x[0], x[0].size()

# 슬라이싱
x[1, : 2, 1 : 3]

"""## view

- 넘파이의 reshape와 동일한 기능임
- 변경 전후 원소 수 동일
- 원소의 순서도 동일
"""

print(f'넘파이 reshape : {x.reshape(2, 2, 6)}')
print(f'텐서 view : {x.view(2, 2, 6)}')

"""## transpose

- 차원 맞교환
"""

# 대표적인 축 교환 : T
print(f'원래 차원 : {x.size()}')
print(f'차원 교환 : {x.T.size()}')

# 특정 차원간의 맞교환 : transpose
print(f'원래 차원 : {x.size()}')
print(f'차원 교환 : {x.transpose(0, 1).size()}')

# 1, 2번 차원이 교환됨

"""## squeeze, unsqueeze

- squeeze : 차원값이 1인 차원 제거
- unsqueeze : 차원값이 1인 차원 추가
"""

# 텐서 생성
x = torch.rand((3, 1, 2, 4, 1))
x.size()

# squeeze
tmp = x.squeeze()
tmp.size()

# 차원값이 1인 차원 제거됨

# unsqueeze
tmp = tmp.unsqueeze(2)
tmp.size()

# 2번 자리에 차원값이 1인 차원이 추가됨

"""## cat, stack

- cat : 단순 합치기
- stack : 쌓아서 합치기
"""

# 텐서 생성
a = torch.rand((2, 3))
b = torch.rand((2, 3))
a.size(), b.size()

# cat
# dim = 0 -> 밑으로 합쳐짐
tmp = torch.cat([a, b], dim = 0)
print(f'dim = 0 : {tmp.size()}')

# dim = 1 -> 옆으로 합쳐짐
tmp = torch.cat([a, b], dim = 1)
print(f'dim = 1 : {tmp.size()}')

# stack
# dim = 0 -> 밑으로 합쳐짐
tmp = torch.stack([a, a, b], dim = 0)
print(f'dim = 0 : {tmp.size()}')

# dim = 1 -> 옆으로 합쳐짐
tmp = torch.stack([a, a, b], dim = 1)
print(f'dim = 1 : {tmp.size()}')

"""## 기초 연산

- 사칙연산은 각 원소 자리에 있는 값들끼리 연산수행
  - 기호 : +, -, *, /
  - 함수
    - torch.add(x, y)
    - torch.sub(x, y)
    - torch.mul(x, y)
    - torch.div(x, y)
- 브로드케스팅
"""

# 텐서 생성
x = torch.Tensor([[1, 2, 3], [4, 5, 6]])
y = torch.Tensor([[10, 20, 30], [4, 5, 6]])

# 사칙연산
print(f'+ : {x + y}')
print(f'- : {x - y}')
print(f'* : {x * y}')
print(f'/ : {x / y}')

# 차원이 다른 텐서(1차원) 생성
z = torch.Tensor([10])

# 브로드캐스팅
x + z
# 서로 차원이 다른데도 (2,3) + (1,)
# 브로드케스팅에 의해 연산에 필요한 만큼 z 텐서를 stretch해서 연산을 수행했음

Image('/content/drive/MyDrive/딥러닝/dl/broadcasting_1.png', width = 600)

Image('/content/drive/MyDrive/딥러닝/dl/broadcasting_2.png', width = 600)

Image('/content/drive/MyDrive/딥러닝/dl/broadcasting_3.png', width = 600)

Image('/content/drive/MyDrive/딥러닝/dl/broadcasting_4.png', width = 600)

"""# 텐서의 기타 함수"""

# 텐서 생성
x = torch.Tensor([1, 2, 3])
y = torch.Tensor([4, 5, 6])

# 내적
# 1 * 4 + 2 * 5 + 3 * 6 = 4 + 10 + 18 = 14 + 18 = 32
torch.dot(x, y)

# 텐서 생성
x = torch.Tensor(
  [
    [1,3],
    [2,3],
    [5,6],
  ]
)
y = torch.Tensor(
  [
    [7,8],
    [9,10]
  ]
)

# 행렬의 곱, xW
# (3, 2) * (2, 2) -> (3, 2)
tmp = torch.mm(x, y)
print(tmp.size())
tmp

