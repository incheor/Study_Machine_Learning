# -*- coding: utf-8 -*-
"""15. 자연어처리#1_단어임베딩_긍정부정예측_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1alVNtcCBVN3Yz8QfefwENZsvFNLcAPHk

# 개요
"""



"""- 자연어
  - 인간이 평소에 말하는 음성, 텍스트
    - 음성 : AI스피커(알렉사,..) 연계
    - 텍스트 : PDF,이미지 상에 글자인식(OCR)
- 자연어 처리
  - **자연어를 컴퓨터가 인식**하고, **처리**하는것
  - Netural Language Processing(NLP)

# 한국어 어려움, 난이도

- 모호성, 다양한 표현이 존재, 불연속 데이터
- 교착어, 띄어쓰기(사람마다 제각각), 신조어/은어(말줄임), 평소문/의문문(?에 따라 뉘앙스도 달라짐)
- 주어도 자주 생략
- 한자기반언어. 한글(표음문자), 한자(표의문자)이로 인한 정보손실 문제도 존재
"""

from IPython.display import Image
Image('/content/drive/MyDrive/k-디지털-품질재단/딥러닝/rnn/같은의미.png', width=500)

# 순서가 달라고 대부분 의미가 동일하다

"""# 절차

- 텍스트 토큰화
  - 텍스트 데이터 -> ... -> 연속적인 백터(수치)
  - **말뭉치(문장덩어리)->분절(성분별로 분해)-> 사전(Dictionary or voca) -> 원-핫인코딩통해백터화 -> 단어 임베딩(압축) -> 정규화(필요시) -> 학습데이터로 준비 완료**

- 분절 수행을 위한 형태소 분석기
  - NLTK(영어권)
  - Konlpy(한국어)
    - Hannanum, kkma, Mecab,...,
    - <a href="https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0">링크</a>
    - 형태소 분석이를 통해 한글의 모호성을 어느정도 해결함
  - 딥러닝 엔진에서도 지원 -> 뉘앙스(NLTK 느낌)

- 용어 정의 
  - 토큰(token) : 단어별, 문장별, 형태소별등등 더이상 나눌수 없는 단위
  - 토큰을 만드는 과정 토큰화(tokenization)
  - 말뭉치(텍스트덩어리)를 더이상 나눌수 없는 작은 단위까지 분해하는 전처리 과정

# 토큰화
"""

# 케라스에서 지원하는 분절 혹은 토큰화 처리 함수
from tensorflow.keras.preprocessing.text import text_to_word_sequence

# 분절의 대상 텍스트
text = '펩 과르디올라 감독이 맨체스터 시티와 동행을 이어갈 예정이다.'

# 토큰화 처리
res = text_to_word_sequence( text )
print( '토큰화후값:', res )
print( '원본데이터:', text )
# 한국어에 적합한 구조는 아님(언어 체계가 다름)

"""- 토큰화 이후 하위 중요한 정보들
  - 수치화 처리
  - 빈도 계산
    - Bag of Words
    - 단어의 가방, 같은 단어를 각각의 가방에 담아서 각 가방에 단어가 몇개 들어 있는지 카운트하는 기법
    - 단어의 빈도는 말뭉치상에서 중요성을 표현하는 단어이다
  - 순서 
"""

with open('/content/drive/MyDrive/k-디지털-품질재단/딥러닝/rnn/NLP_bow.txt') as f:
  docs = f.readlines()
docs

from tensorflow.keras.preprocessing.text import Tokenizer

# 토근화 함수 생성
token = Tokenizer()        # 토큰화 처리를 하는 객체 생성
token.fit_on_texts(docs)   # 토큰화 처리 진행 -> 빈도계산 -> 
                           # 학습을 통해서 중요도체크

# 단어의 빈도를 묶어서 출력 
print( token.word_counts )

# 기타 정보
print('문장(원문의 문자의) 카운트', token.document_count)
print('각 단어가 몇개의 문자에 포함되어 있는가', token.word_docs)
# 사전에 각 단어가 몇번인지 세팅
print('각 단어에 세팅된 인덱스값', token.word_index)

"""# 텍스트의 백터화(원-핫  인코딩)"""

text = '펩 과르디올라 감독이 맨체스터 시티와 동행을 이어갈 예정이다.'
token= Tokenizer()
token.fit_on_texts( [text] ) 
print( token.word_index )

# 텍스트 토큰화를 통해 문자을 수치로 표현했다
# 사전에 세팅된 단어의 인덱스 번호로 문자을 표현
x = token.texts_to_sequences( [text] )
print( x )

# 수치로 표현된 문장을 원-핫인코딩으로 처리
# 인코딩되고 나서 맨앞의 숫자가 1이 되면 않된다 Rule 
# -> 사전의등록된단어수 + 1 => 위의 룰을 지킬수 있다
from tensorflow.keras.utils import to_categorical

x = to_categorical( x, len(token.word_index)+1 )
print( x )

"""- 원-핫 인코딩 문제
  - 인코딩후 그대로 사용 -> 말뭉치가 커질수록 백터의 길이도 커진다
  - 10만단어로 만들어진 사전 -> 원-핫 인코딩 -> (10만 + 1)개 feature가 생성된다
  - 해결방법
    - 단어 임베딩(word embeding)
- 단어 임베딩
  - 원리 
    - 주어진 배열(백터)을 정해진 길이로 압축
    - 이를 위해서 각 단어의 유사도를 계산하여 처리
      - happy는 bad보다 good에 더 가깝다
  - 아래 그림은 feature 16을 4에 임베딩한 예시
  - 대부분 딥러닝엔진이 지원
"""

Image('/content/drive/MyDrive/k-디지털-품질재단/딥러닝/rnn/원-핫인코딩vs단어임베딩.png')

Image('/content/drive/MyDrive/k-디지털-품질재단/딥러닝/rnn/단어유사도.png')
# 단어의 유사도 샘플

"""# 패딩

- 보정
- 말뭉치중 문장들은 제각각의 크기를 가진다
- 현재까지는 단어들을 동일크기로 인코딩 처리했다
- 문장은 단어들로 구성되어 있고, 문장의 길이는 제각각이다(토큰의 수도 제각각)
- 동일크기로 문장 데이터를 표현하자
"""

docs = [
  "아 더빙.. 진짜 짜증나네요 목소리",
  "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나",
  "너무재밓었다그래서보는것을추천한다",
  "교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정",
  "사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다",
  "막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.",
]
labels = [ 0, 1, 0, 0, 1, 0]

token= Tokenizer()
token.fit_on_texts( docs ) 
print( token.word_index )

x = token.texts_to_sequences( docs )
x
# 모든 문장은 수치로 표현되었지만, 크기가 제각각이다

max_len = 0
for doc in x:
  if len(doc) > max_len:
    max_len = len(doc)
max_len

from tensorflow.keras.preprocessing.sequence import pad_sequences

# 문장의 개수상 비워있는 자리는 모두 0으로 채움
padding_x = pad_sequences(x, max_len)
padding_x

# 맨 앞자리는 0으로 채운다
padding_x = pad_sequences(x, max_len+1)
padding_x
# 파이토치에서는 아래처럼 설정해서(사전에 추가) 백터화하기도 한다
# <pad>  => 패딩값 => 0
# <nan>  => 사전에 없는 토큰 => 1

"""# 텍스트를 읽어서 긍정 부정 예측"""

from tensorflow.keras.models import Sequential
# Embedding 층은 레이어로 존재
from tensorflow.keras.layers import Dense, Flatten, Embedding

# 토큰의 총개수 + 1
# 임베딩시 현재 준비된 사전의 총개수 + 1
word_size = len(token.word_index) + 1
word_size

"""## 신경망 구성

### 단어 임베딩
"""

# 네트워크 구성
model = Sequential()
# (word_size)42개를 넣어서 백터크기 8개로 문자을 표현한다(임베딩한다)
# (None. 4, 8 )
model.add( Embedding( word_size, 8, input_length=(max_len+1) ) )
model.add( Flatten()   )
model.add( Dense(1, activation='sigmoid') )
# 모델의 신경망 shape 요약
model.summary()
# 학습하면서 조정할 파라미터 441개

import numpy as np

"""## 학습,예측"""

# 컴파일
model.compile(optimizer='adam', 
              loss='binary_crossentropy', metrics=['accuracy']  )
# 훈련
model.fit( padding_x, np.array(labels), epochs=20 )

# 예측 -> 일단 이미 학급한 데이터를 사용하겟다(임시)
print('acc %.4f' % ( model.evaluate( padding_x, np.array(labels) )[1]  ) )

"""---

# 문제점 확인

- 영어권의 언어와 한국어는 구성, 의미도 다름
- 한국어 적합한 토큰화를 위한 형태소 분석기를 개입시켜서 구성

---

# konlpy기반 하위 형태소 분석기 사용 -  MeCab
"""

Image('/content/drive/MyDrive/k-디지털-품질재단/딥러닝/rnn/분절.png')

"""## 설치"""

!pip install konlpy

!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)

import os
os.chdir('/tmp/')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.1.tar.gz
!tar zxfv mecab-0.996-ko-0.9.1.tar.gz
os.chdir('/tmp/mecab-0.996-ko-0.9.1')
!./configure
!make
!make check
!make install

import os
os.chdir('/tmp')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz
!tar -zxvf mecab-ko-dic-2.0.1-20150920.tar.gz
os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')
!./autogen.sh
!./configure
!make
# !sh -c 'echo "dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic" > /usr/local/etc/mecabrc'
!make install

# install mecab-python
import os
os.chdir('/content')

!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git
os.chdir('/content/mecab-python-0.996')

!python3 setup.py build
!python3 setup.py install|

"""## 맥캡 사용"""

from konlpy.tag import Mecab

# 텍스트
ko_sentence = '오늘 점심 무엇으로 먹을까' 
ko_sentence

# 한국어를 위한 형태소 분석기
tokenizer = Mecab()

tokenizer.morphs( ko_sentence )
# 토큰을 획득했는데, 성분을 모르겟다

# 토큰의 성분까지 확인 => 특정 성분을 제거하고 문장을 재구성 가능
tokenizer.pos( ko_sentence )

# 성분 의미 체크
tokenizer.tagset.get('MAG'), tokenizer.tagset.get('NNG')

"""## 사전작업(원-핫인코딩, 패딩)

- ratings_train(text).txt 읽어서 DataFrame으로 구성
- 텍스트 파트를 MeCab으로 토큰화 다시 문장으로 조립 => 구분자는 공백 => ' '
- 이 데이터를 가지고 신경망에 주입해서 학습
"""

# 대량 데이터 
train_txt = '/content/drive/MyDrive/k-디지털-품질재단/딥러닝/rnn/ratings_train.txt'
test_txt  = '/content/drive/MyDrive/k-디지털-품질재단/딥러닝/rnn/ratings_test.txt'

# txt -> df 구성 -> read_csv() 이용 -> 상위 2개만 출력
import pandas as pd
df_train = pd.read_csv(train_txt, sep='\t', engine='python')
df_train.head(2)

# document -> 하나씩 꺼내서 -> Mecab으로 토큰화 
#          -> ' '를 구분자로 문자열통합 -> 대체 => apply()
def meCabTokenizerToStr( x ):
  # 결측치에 대해서는 ""일반 문자열로 대체
  if not x: return ''
  try:
    return ' '.join( tokenizer.morphs( x ) )
  except Exception as e:
    print( e )
    return ''
  pass

df_train.document = df_train.document.apply(meCabTokenizerToStr)
df_train.head(2)

# 케라스에서 지원하는 토큰화 처리
token = Tokenizer()
token.fit_on_texts( df_train.document )

# 사전의 총개수
print( len( token.word_index ) )
print( token.word_index )

# 문장을 수치로 표현
x = token.texts_to_sequences( df_train.document )

x[:2]

# 최대 문장 길이 계산
max_len = 0
for doc in x:
  if len(doc) > max_len:
    max_len = len(doc)
max_len

# 패팅 -> 문장을 모두 같은크기로 세팅 (백터의 크기 통일)
padded_x = pad_sequences( x, max_len+1 )
padded_x.shape, padded_x

# 임베딩에 입력될 단어수 => 원핫인코딩 고려 + 1
# 사전의 총 토큰수 : len( token.word_index )
word_size = len( token.word_index ) + 1
word_size

"""## 신경망 작성

### 단어임베딩
"""

model = Sequential()
model.add( Embedding( word_size, 8, input_length=(max_len+1) ) )
model.add( Flatten()   )
model.add( Dense(1, activation='sigmoid') )
model.summary()

"""## 학습"""

padded_x.shape, df_train.label.shape

model.compile(optimizer='adam', 
              loss='binary_crossentropy', metrics=['accuracy']  )
model.fit( padded_x, df_train.label , epochs=20 )

"""## 테스트"""

# 테스트 데이터 준비 -> 테스트 데이터의 토큰이 사전에 포함되어 있지 않는
# 비율이 크다면 -> 정확도는 많이 떨어질것이다 -> 지속적인 사전 업데이터 작업 필요
df_test          = pd.read_csv(test_txt, sep='\t', engine='python')
df_test.document = df_test.document.apply(meCabTokenizerToStr)

df_test.head(5)
# 빈내용, 외계어(특수문자로된), 영어등등 제거가 좋을듯

x = token.texts_to_sequences( df_test.document )
x[:2]
# 영어로된 문장을 사전에 없었으므로 수치화 과정에서 [] 으로 처리되었다

max_len = 0
for doc in x:
  if len(doc) > max_len:
    max_len = len(doc)
max_len

# 테스트 데이터의 문장의 크기로 계산하면 않됨
#padded_x = pad_sequences( x, max_len+1 )

# 모델의 잣대에 맞춰서 구성 -> 네트워크를 통과할수 있다 -> 정보손실이 예상됨
# 테스트 문장이 더 길다면 정보손실 발생
padded_x = pad_sequences( x, 117 )

# 예측
print('acc %.4f' % ( model.evaluate( padded_x, df_test.label  )[1]  ) )

# 결론 
# 정확도를 높이기 위해서 사전을 더 많이 확보해야 겟다
# 요구사항의 변화에 따라 신경망을 교체
# 뉴스카테고리 분석, 리뷰 분류, 문장안에서 토큰의 좌우 관계 고려, 
# 의미에대해서 좀더 접근, 토픽 추출등, 연관성, 빈도, 순서(시퀀스)등을 고려 
# RNN, LSTM으로 신경망을 교체해서 진행